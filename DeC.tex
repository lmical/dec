\documentclass[11pt,a4paper,oneside,openright]{book}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
%\usepackage[italian]{babel}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{pdfpages}


\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{observation}{Observation}
\newtheorem{proposition}{Proposition}
\newtheorem{cor}{Corollary}

\newcommand{\prob}[0]{P}

\newcommand{\lop}[0]{\mathcal{L}}
\newcommand{\lopd}[0]{\mathcal{L}_\Delta}
\newcommand{\lopdt}[0]{\mathcal{L}_{\Delta}}
\newcommand{\usol}[0]{\underline{\uvec{u}}_\Delta}
\newcommand{\usoldt}[0]{\underline{\uvec{u}}_\Delta}

\newcommand{\uex}[0]{\underline{\uvec{u}}^{ex}}
\newcommand{\up}[0]{\underline{\uvec{u}}^{(p)}}

\newcommand{\usoldto}[0]{\tilde{\underline{\uvec{u}}}_\Delta}

\newcommand{\uapp}[0]{\uvec{u}_h}

\newcommand{\massmatrix}[0]{\mathcal{M}}

\newcommand{\tess}[0]{\mathcal{T}_h}


%\newcommand{\uvec}[2][3]{\underline{#2\mkern-#1mu}\mkern#1mu}
\newcommand{\uvec}[2][3]{\mathbf{#2\mkern-#1mu}\mkern#1mu}

\newcommand{\res}[0]{\textbf{R}}
\newcommand\norm[1]{\left\lVert#1\right\rVert}

\newcommand{\flux}[0]{\mathbf{F}}
\newcommand{\source}[0]{\mathbf{S}}
\newcommand{\ST}[0]{\mathbf{ST}_i^K}

\newcommand{\elres}[0]{\uvec{\Phi}^K(t,\uapp)}
\newcommand{\noderes}[0]{\uvec{\Phi}^K_i(t,\uapp)}


\def\restriction#1#2{\mathchoice
              {\setbox1\hbox{${\displaystyle #1}_{\scriptstyle #2}$}
              \restrictionaux{#1}{#2}}
              {\setbox1\hbox{${\textstyle #1}_{\scriptstyle #2}$}
              \restrictionaux{#1}{#2}}
              {\setbox1\hbox{${\scriptstyle #1}_{\scriptscriptstyle #2}$}
              \restrictionaux{#1}{#2}}
              {\setbox1\hbox{${\scriptscriptstyle #1}_{\scriptscriptstyle #2}$}
              \restrictionaux{#1}{#2}}}
\def\restrictionaux#1#2{{#1\,\smash{\vrule height .8\ht1 depth .85\dp1}}_{\,#2}} 






\begin{document}

\newgeometry{top=1cm, bottom=1cm}
\thispagestyle{empty}
\begin{center}
\includegraphics[width=5cm]{frontespizio.png}\\
\vspace{1.5cm}
{\bf Universität Zürich}\\
{\vspace{0.5cm}}
Institut für Mathematik\\
{\vspace{0.5cm}}
\vspace{1cm}
\rule{15cm}{.4pt}
\vspace{1.5cm} \\
{\bf Lorenzo Micalizzi}\\
\vspace{1.7cm}
{\bf Deferred Correction:} \\
{\bf from simple integration of systems of ordinary differential equations to explicit time-integration in Residual Distribution and Discontinuous Galerkin schemes for balance laws} \\
\vspace{1.7cm}
\vspace{1.2cm}

\vspace{0.8cm}
{\bf Prof. Remi Abgrall} \\
\vspace{1cm}
\rule{15cm}{.4pt}\\
\vspace{1cm}
{\bf November 2020}
\end{center}
\restoregeometry
\newpage
\tableofcontents
\listoffigures

\chapter{Abstract} 
This work is an attempt to give a systematic introduction to the Deferred Correction method which provides an iterative procedure to get arbitrary high-order integration schemes for systems of ordinary differential equations.
We can also apply the Deferred Correction in the context of Residual Distribution and Finite Elements methods for hyperbolic systems of balance laws to get an explicit formulation and avoid the inversion of mass matrices.
The structure of this script is the following:
\begin{itemize}
\item[•] We start by introducing the Deferred Correction as a general method to approximate the solution of a general "tough" operator $\lopd^2$ with an arbitrary high-order precision through an iterative procedure which make use of the operator $\lopd^1$ which is a "naive" operator.
Typically the "tough" operator $\lopd^2$ is an implicit high order operator difficult to solve while the "naive" operator $\lopd^1$ is an explicit low order operator easy to solve.
\item[•] Then we apply this method to the context of the integration of systems of ordinary differential equations.
\item[•] In order to apply the Deferred Correction to some numerical methods for solving hyperblic systems of balance laws, we present a small introduction of the analytical problem. We derive the balance laws and we introduce their weak formulation in order to overcome the limits of the classical one.
\item[•] Thus we introduce from a general point of view the semidiscrete formulation which is a framework common to many numerical methods for hyperbolic systems of balance laws (like Residual Distribution and Discontinuous Galerkin Finite Elements) and focus on the problem of the inversion of the mass matrix.
\item[•] Hence we present the Residual Distribution approach and see how to apply the Deferred Correction in this context where avoiding the inversion of the mass matrix without spoiling the accuracy of the scheme plays a crucial role.
\item[•] Therefore we introduce the Discontinuous Galerkin Finite Elements method and show how the Deferred Correction can be enforced also in this case.
\item[•] In the end we show how the Deferred Correction method as an ODE solver can be seen as a particular Runge-Kutta scheme.
\end{itemize} 
\chapter[The Deferred Correction in an abstract framework]{The Deferred Correction as a general procedure in an abstract framework}
%\chapter{The Deferred Correction as a general procedure in an abstract framework}
We will first introduce the Deferred Correction in an abstract context.
Assume that we have two general operators depending on a parameter\footnote{This parameter represents the time-step in the context of the integration of ordinary differential equations, while it represents the characteristic mesh size in the application to hyperbolic balance laws. The esteems that we will get will be strictly depending on this parameter which has to be fixed in advance. In other words we are able to prove the arbitrary high-order consistency for a fixed $\Delta$ and what happens in the mesh refinment is still unclear from a theoretical point of view even if numerical experiments seem to confirm the desired order of convergence.} $\Delta$ between two normed vector spaces $\left(X, \norm{\cdot}_X \right)$ and $\left(Y, \norm{\cdot}_Y \right)$
$$\lopd^1,\lopd^2:X \longrightarrow Y.$$
\begin{observation}
Even if we are still in an abstract context and not in the specific case of an evolution problem (like for example an ordinary differential equation) it is useful in order to make things clearer to give an idea of the objects we are working with.
Imagine that we want to solve numerically a Cauchy problem for a system of ODEs, then $\lopd^2$ is an high-order implicit operator and $\lopd^1$ is an explicit low-order operator (for example the operator that we get by using the Euler approximation).
We would like to solve $\lopd^2$ i.e. finding $\uvec{u}\in X$ such that $\lopd^2(\underline{\uvec{u}})=\uvec{0}_Y$ but this is not so easy given the implicit nature of the operator. On the other hand the other exlicit operator $\lopd^1$ is very easy to solve\footnote{More in general it is easy to solve $\lopd^1=\underline{\uvec{v}}$ with $\underline{\uvec{v}} \in Y$ i.e. finding $\underline{\uvec{u}}\in X$ such that $\lopd^1(\underline{\uvec{u}})=\underline{\uvec{v}}$.} but it is a low-order operator. 
\end{observation}
This observation was meant to provide a better understanding of the meaning of the operators $\lopd^1$ and $\lopd^2$ and of the direction that we are going to take. Let's now go back to our general framework, we focus more in detail later on the application to the evolution problems. 
For the moment just imagine that we have to solve $\lopd^2$ because it provides a more accurate solution but this operator is difficult to solve. We would like to solve $\lopd^1$ instead of $\lopd^2$ because it is easier to solve but it is not enough accurate.
In the next theorem we will provide a receipt to get a high-order approximation of the solution of $\lopd^2$ in an explicit way by combining the operators $\lopd^1$ and $\lopd^2$ through an iterative procedure.
\begin{theorem} \textbf{Deferred Correction}\\
If 
\begin{itemize}
\item[i)] \textbf{Existence of a unique solution to $\lopd^2$} \\
$\exists ! \usol \in X$ solution of $\lopd^2$ i.e. such that $\lopd^2(\usol)=\uvec{0}_Y$;
\item[ii)] \textbf{Sub-coercivity of $\lopd^1$} \\
$\exists \alpha_1 \geq 0$ independent of $\Delta$ s.t. $$\norm{\lopd^1(\underline{\uvec{v}})-\lopd^1(\underline{\uvec{w}})}_Y\geq \alpha_1\norm{\underline{\uvec{v}}-\underline{\uvec{w}}}_X \quad \forall \underline{\uvec{v}},\underline{\uvec{w}}\in X;$$
\item[iii)] \textbf{Lipschitz-continuity-like condition of $\lopd^1-\lopd^2$} \\
$\exists \alpha_2 \geq 0$ independent of $\Delta$ s.t. $$\norm{\left(\lopd^1(\underline{\uvec{v}})-\lopd^2(\underline{\uvec{v}})\right)-\left(\lopd^1(\underline{\uvec{w}})-\lopd^2(\underline{\uvec{w}})\right)}_Y\leq \alpha_2 \Delta \norm{\underline{\uvec{v}}-\underline{\uvec{w}}}_X \quad \forall \underline{\uvec{v}},\underline{\uvec{w}}\in X.$$
\end{itemize}
then if we consider
\begin{equation}
\label{DeC}
\lopd^1(\underline{\uvec{u}}^{(p)})=\lopd^1(\underline{\uvec{u}}^{(p-1)})-\lopd^2(\underline{\uvec{u}}^{(p-1)}) \quad p=1,2,\dots,P
\end{equation}
we have that
\begin{equation}
\label{accuracyestimate}
\norm{\underline{\uvec{u}}^{(P)}-\usol}_X \leq \left( \Delta \frac{\alpha_2}{\alpha_1} \right)^P\norm{\underline{\uvec{u}}^{(0)}-\usol}_X.
\end{equation}
\end{theorem}
\begin{proof}
By using the sub-coercivity of $\lopd^1$ we have
\begin{equation}
\label{eq:2}
\norm{\underline{\uvec{u}}^{(P)}-\usol}_X\leq \frac{1}{\alpha_1}  \norm{\lopd^1(\underline{\uvec{u}}^{(P)})-\lopd^1(\usol)}_Y.
\end{equation}
Let's focus on the right side of this inequality. By definition of $\lopd^1(\underline{\uvec{u}}^{(p)})$ for $p=1,2,\dots,P$ in equation \ref{DeC}, we have
$$\frac{1}{\alpha_1}  \norm{\lopd^1(\underline{\uvec{u}}^{(P)})-\lopd^1(\usol)}_Y = \frac{1}{\alpha_1}  \norm{\lopd^1(\underline{\uvec{u}}^{(P-1)})-\lopd^2(\underline{\uvec{u}}^{(P-1)})-\lopd^1(\usol)}_Y$$
and since $\usol$ is the solution of $\lopd^2$ we have that $\lopd^2(\usol)=\uvec{0}_Y$ and we can add it on the right side and get
$$\frac{1}{\alpha_1}  \norm{\lopd^1(\underline{\uvec{u}}^{(P-1)})-\lopd^2(\underline{\uvec{u}}^{(P-1)})-\lopd^1(\usol)}_Y=\frac{1}{\alpha_1}  \norm{\lopd^1(\underline{\uvec{u}}^{(P-1)})-\lopd^2(\underline{\uvec{u}}^{(P-1)})-\lopd^1(\usol)+\lopd^2(\usol)}_Y.$$
Thus the initial inequality \ref{eq:2} becomes 
\begin{equation}
\label{eq:3}
\norm{\underline{\uvec{u}}^{(P)}-\usol}_X\leq \frac{1}{\alpha_1}  \norm{\left[\lopd^1(\underline{\uvec{u}}^{(P-1)})-\lopd^2(\underline{\uvec{u}}^{(P-1)})\right]-\left[\lopd^1(\usol)-\lopd^2(\usol)\right]}_Y.
\end{equation}
Thanks to the Lipschitz-continuity-like condition we can write
$$\frac{1}{\alpha_1}  \norm{\left[\lopd^1(\underline{\uvec{u}}^{(P-1)})-\lopd^2(\underline{\uvec{u}}^{(P-1)})\right]-\left[\lopd^1(\usol)-\lopd^2(\usol)\right]}_Y \leq \Delta \frac{\alpha_2}{\alpha_1}\norm{\underline{\uvec{u}}^{(P-1)}-\usol}_X.$$
And thus \ref{eq:3} becomes
\begin{equation}
\label{eq:4}
\norm{\underline{\uvec{u}}^{(P)}-\usol}_X\leq \Delta \frac{\alpha_2}{\alpha_1}\norm{\underline{\uvec{u}}^{(P-1)}-\usol}_X.
\end{equation}
By repeating these calculations recursively we get the thesis.
\end{proof}
The result is very general and may seem a little abstract but let's notice that if the operator $\lopd^1$ is "easy" to solve (as it will be) then \ref{DeC} represents a simple receipt to approximate arbitrarily well the solution $\usol$ of $\lopd^2$. The convergence to the solution for $P \rightarrow +\infty$ is ensured independently of the starting vector $\uvec{u}^{(0)}$ provided that $\Delta \frac{\alpha_2}{\alpha_1}<1$. 
Let's make some final observations.
\begin{observation}
\label{optimaliterations}
If the solution $\usol$ of $\lopd^2$ is a $R$-th order approximation of the exact solution $\underline{\uvec{u}}^{ex}$ of a more general problem then a $R$-th order approximation $\underline{\tilde{\uvec{u}}}$ of $\usol$ is a $R$-th order approximation of the exact solution $\underline{\uvec{u}}^{ex}$ as well. It follows easily by applying the triangular inequality
$$\norm{\underline{\tilde{\uvec{u}}}-\underline{\uvec{u}}^{ex}}_X \leq \norm{\underline{\tilde{\uvec{u}}}-\usol}_X+\norm{\usol-\underline{\uvec{u}}^{ex}}_X\leq O(\Delta^{R+1})+O(\Delta^{R+1})=O(\Delta^{R+1}).$$
Thus in this case we have to use the Deferred Correction procedure to approximate $\usol$ exactly with $R$-th order accuracy, any other extra precision in approximating $\usol$ would be useless\footnote{In the context of the integration of ordinary differential equations, the solution $\usol$ of $\lopd^2$ will be a $(M+1)$-th order accurate approximation of the exact solution $\underline{\uvec{u}}^{ex}$ and we will perform $P=M+1$ iterations to get $\underline{\uvec{u}}^{(M+1)}$ which will be a $(M+1)$-th order accurate approximation of $\usol$ and thus of $\underline{\uvec{u}}^{ex}$.}. We will be more precise about the accuracy and the number of iterations needed to reach the highest possible accuracy later.
\end{observation}
\begin{observation} %%%%%%%%%%TRUE?
The independecy of the coefficients $\alpha_1$ and $\alpha_2$ may naively lead us to think that we can easily consider the limit for $\Delta \rightarrow 0$ but this is not true at least from a theoretical point of view. In fact the whole framework has a non-trivial dependence on $\Delta$: the operators $\lopd^1$ and $\lopd^2$ as well as the spaces $X$ and $Y$ and the norms on them depend on $\Delta$. Despite this the numerical experiments seem to confirm the desired order of convergence.
\end{observation}
Now that we have introduced and proved the Deferred Correction in a general framework, the only thing that it is left to do to apply it in a more specific context is to characterize the objects needed (the operators $\lopd^1$ and $\lopd^2$, the spaces $X$ and $Y$, the norms) and verify that the three hypoteses are satisfied.
\chapter{The Deferred Correction for systems of ODEs}
We will now apply the Deferred Correction method to the context of the integration of systems of ordinary differential equations.
Let's consider the Cauchy problem
\begin{equation}
\label{ODE}
\begin{cases}
\frac{d}{dt}\uvec{u}(t) = \uvec{G}(t,\uvec{u}(t)) \\
\uvec{u}(t_0)=\uvec{u}_0
\end{cases}
\end{equation}
with $\uvec{u} \in \mathbb{R}^N$ and the usual hypoteses of regularity which ensure the existence of a unique solution i.e. $\uvec{G}$ continuous and Lipschitz-continuous with respect to $\uvec{u}$ uniformly with respect to $t$ with a Lipschitz constant $L$.
We would like to approximate numerically the solution at the time $t_0+\Delta t$.
We are now going to put the problem in a Deferred Correction framework.
Clearly in this case the parameter $\Delta$ is the time step $\Delta t$.
Let's now define the operators $\lopdt^1$ and $\lopdt^2$ and the normed vector spaces $X$ and $Y$.
\section{Derivation of the operators $\lopdt^1$ and $\lopdt^2$}
As anticipated in the abstract context $\lopdt^2$ is a "tough" high-order implicit operator which provides the desired accuracy but that is difficult to solve, instead $\lopdt^1$ is a "naive" low-order explicit operator that is easy to solve but that is not enough accurate.
\subsection{Derivation of $\lopdt^2$}
We are interested in an approximate solution at the time $t_0+\Delta t$ but in order to get it we will introduce a set of nodes in the interval the interval $[t_0,t_0+\Delta t]$ where we will consider an approximation of our sytem of ODES \ref{ODE}.
The first node will coincide with $t_0$, the last one with $t_0+\Delta t$. Clearly we are interested just in the approximation of the solution in the last node but we will need also the approximations of the solution in the other nodes.



We consider the interval $[t_0,t_0+\Delta t]$ and define $M+1$ nodes $t^m$ with $m=0,1,\dots,M$ such that 
$$t_0=t^0<t^1<\dots<t^M=t_0+\Delta t$$
like in figure \ref{timesubintervals}. 
\begin{observation}
We can assume the nodes to be equispaced for simplicity's sake but this is not mandatory and the procedure would not change for any other arbitrary distribution of them. Anyway if we assume an equispaced distribution, the distance between two consecutive points is $\frac{\Delta t}{M}$.
\end{observation}
\begin{figure}[hp] \centering{\includegraphics[scale=0.5]{timesubintervals.pdf}}
\caption{Nodes in the interval $[t^0,t^0+\Delta t]$.}
\label{timesubintervals}
\end{figure}

%\begin{figure}[h]
%\centering
%\includegraphics[width=0.7\textwidth]{timesubintervals.png}
%\caption{Nodes in the interval $[t^0,t^0+\Delta t]$} 
%\label{timesubintervals}
%\end{figure}

In each one of these nodes we will refer to $\uvec{u}(t^m)$ as the exact solution in $t^m$ and to $\uvec{u}^m$ as the approximate solution in the same node.
Just for the first node we set $\uvec{u}^0=\uvec{u}(t_0)=\uvec{u}_0$ without any approximation, but obviously the exact soutions as well as the approximate solutions in the other nodes are unkonwns.


An exact integration of the system of ODEs would result in
\begin{equation}
\label{exint}
\uvec{u}(t^m)-\uvec{u}^0-\int_{t^0}^{t^m}\uvec{G}(t,\uvec{u}(t))dt=\uvec{0}
\end{equation}
from which we would have the exact solution $\uvec{u}(t^m)$.
Unfortunately we cannot perform in general the exact integration (also because we actually do not know $\uvec{u}$ in $[t^0,t^m]$) and we need to make some approximations. Let's approximate $\uvec{G}(t,\uvec{u}(t))$ with $M$-order accuracy through the Lagrange polynomials of degree $M$ corresponding to the $M+1$ nodes $t^m$ with $m=0,1,\dots,M$ i.e.
\begin{equation}
\label{approx}
\uvec{G}(t,\uvec{u}(t)) = \sum_{l=0}^{M} \uvec{G}(t^l,\uvec{u}(t^l))\psi^l(t)+O(\Delta t^{M+1})
\end{equation}
with
$$\psi^l(t)=\prod_{\substack{m=0 \\ m \neq l}}^M \frac{(t-t^m)}{(t^l-t^m)} \quad \forall l=0,1,\dots,M.$$
In figure \ref{lagrange} are shown the Lagrange polynomials corresponding to equispaced nodes in the reference interval $[0,1]$ from order $1$ to $6$.

%%%%%%%%%%%%%%%%%Image of Lagrangian polynomials?
%\begin{figure}[h] \centering{\includegraphics[scale=0.3]{lagrange.png}}
%\caption{Lagrange polynomials in the reference interval $[0,1]$.}
%\label{lagrange}
%\end{figure}
\begin{figure}[h] \centering{\includegraphics[scale=0.5]{lagrange.pdf}}
\caption{Lagrange polynomials in the reference interval $[0,1]$.}
\label{lagrange}
\end{figure}



Thus if we substitute the approximation \ref{approx} in the formula got by the exact integration \ref{exint} we get a relation involving $\uvec{u}^m$ which is a $M+1$ order accurate approximation of $\uvec{u}(t^m)$
\begin{equation}
\label{almosttrue}
\uvec{u}^m-\uvec{u}^0-\int_{t^0}^{t^m}\sum_{l=0}^{M} \uvec{G}(t^l,\uvec{u}(t^l))\psi^l(t)dt=\uvec{0}.
\end{equation}
\begin{observation}
We cannot use this relation to calculate $\uvec{u}^m$ because we do not know the values $\uvec{u}(t^l)$. So we have not finished yet and \ref{almosttrue} is not yet our final operator $\lopdt^2$. We need to midify it a bit.
\end{observation}
Before modifying \ref{almosttrue} to get something that we can actually use let's verify the claim on its $M+1$-order accuracy. 
\begin{proposition}
$\uvec{u}^m$ satisfying \ref{almosttrue} is a $M+1$ order accurate approximation of $\uvec{u}(t^m)$.
\end{proposition}
\begin{proof}
Let's compute $\uvec{u}(t^m)-\uvec{u}^m$ with $\uvec{u}^m$ got by \ref{almosttrue}. From \ref{exint}, \ref{almosttrue} and the $M$-order accuracy on the approximation of $\uvec{G}(t,\uvec{u}(t))$ i.e. equation \ref{approx} we have
$$\uvec{u}(t^m)-\uvec{u}^m=\uvec{u}^0+\int_{t^0}^{t^m}\uvec{G}(t,\uvec{u}(t))dt-\uvec{u}^0-\int_{t^0}^{t^m}\sum_{l=0}^{M} \uvec{G}(t^l,\uvec{u}(t^l))\psi^l(t)dt=$$
$$
=\int_{t^0}^{t^m} \left[ \uvec{G}(t,\uvec{u}(t))-\sum_{l=0}^{M} \uvec{G}(t^l,\uvec{u}(t^l))\psi^l(t) \right]dt=\int_{t^0}^{t^m}O(\Delta t^{M+1})dt=O(\Delta t^{M+2}).$$
\end{proof}

As we said before we do not have the values $\uvec{u}(t^l)$ and we need to modify \ref{almosttrue} to get somthing which can actually be used to get $\uvec{u}^m$. 

The idea is to use $\uvec{u}^l$ in place of them, thus getting an implicit formulation. By doing this we do not lose any accuracy as we will see in a few lines. The implicit formulation that allows us to get $\uvec{u}^m$ with $M+1$-order accuracy reads then
\begin{equation}
\label{true}
\uvec{u}^m-\uvec{u}^0-\int_{t^0}^{t^m}\sum_{l=0}^{M} \uvec{G}(t^l,\uvec{u}^l)\psi^l(t)dt=\uvec{0} \quad \forall m=1,2,\dots,M
\end{equation}
This is the operator $\lopdt^2$ we were looking for but before going ahead and rearrange it in a more clever way let's verify that, despite the extra approximation introduced, the order of accuracy is still $M+1$.
\begin{proposition}
$\uvec{u}^m$ satisfying \ref{true} is a $M+1$-order accurate approximation of $\uvec{u}(t^m)$.
\end{proposition}
\begin{proof}
Let's first show that, by substituiting $\uvec{u}^l$ to $\uvec{u}(t^l)$, we are making an $O(\Delta t^{M+2})$ approximation on $\sum_{l=0}^{M} \uvec{G}(t^l,\uvec{u}(t^l))\psi^l(t)$.
By applying the triangular inequality and remembering that the Lagrangian functions are bounded by a constant\footnote{Once that the nodes are fixed the Lagrangian functions are continuous functions over a compact set and for the Weierstrass theorem they are bounded.}, say $C$, we get
$$\norm{  \sum_{l=0}^{M} \left[ \uvec{G}(t^l,\uvec{u}^l)- \uvec{G}(t^l,\uvec{u}(t^l)) \right]  \psi^l(t)}_\infty \leq C \sum_{l=0}^{M} \norm{\uvec{G}(t^l,\uvec{u}^l)- \uvec{G}(t^l,\uvec{u}(t^l))}_\infty$$
and due to the Lipschitz-continuity of the flux with respect to $\uvec{u}$ uniformly with respect to $t$ with Lispchitz constant $L$ and the fact that $\uvec{u}^l$ is an $O(\Delta t^{M+2})$ order approximation of $\uvec{u}(t^l)$ we get
$$C \sum_{l=0}^{M} \norm{\uvec{G}(t^l,\uvec{u}^l)- \uvec{G}(t^l,\uvec{u}(t^l))}_\infty \leq C L \sum_{l=0}^{M} \norm{\uvec{u}^l-\uvec{u}(t^l)}_\infty \leq CL \sum_{l=0}^{M} O(\Delta t^{M+2})=O(\Delta t^{M+2}).$$
In the end we get
\begin{equation}
\label{accurate}
\sum_{l=0}^{M} \left[ \uvec{G}(t^l,\uvec{u}^l)- \uvec{G}(t^l,\uvec{u}(t^l)) \right]  \psi^l(t)=O(\Delta t^{M+2})
\end{equation}
Thus by substituting $\uvec{u}^l$ to $\uvec{u}(t^l)$ we are making an $O(\Delta t^{M+2})$ approximation in the evaluation of $G(t,\uvec{u}(t))$ which doesn't affect the accuracy. In fact if we consider $\uvec{u}(t^m)-\uvec{u}^m$ with $\uvec{u}^m$ got by equation \ref{true}, remembering \ref{exint}, we have
$$\uvec{u}(t^m)-\uvec{u}^m=\uvec{u}^0+\int_{t^0}^{t^m}\uvec{G}(t,\uvec{u}(t))dt-\uvec{u}^0-\int_{t^0}^{t^m}\sum_{l=0}^{M} \uvec{G}(t^l,\uvec{u}^l)\psi^l(t)dt=$$
$$=\int_{t^0}^{t^m} \left[ \uvec{G}(t,\uvec{u}(t))-\sum_{l=0}^{M} \uvec{G}(t^l,\uvec{u}^l)\psi^l(t) \right]dt.$$
Recalling now \ref{accurate} we have
$$\int_{t^0}^{t^m} \left[ \uvec{G}(t,\uvec{u}(t))-\sum_{l=0}^{M} \uvec{G}(t^l,\uvec{u}^l)\psi^l(t) \right]dt=\int_{t^0}^{t^m} \left[ \uvec{G}(t,\uvec{u}(t))-\sum_{l=0}^{M} \uvec{G}(t^l,\uvec{u}(t^l))\psi^l(t)+O(\Delta t^{M+2}) \right]dt$$
and by using again \ref{approx} we have our desired result on the accuracy
$$\int_{t^0}^{t^m} \left[ \uvec{G}(t,\uvec{u}(t))-\sum_{l=0}^{M} \uvec{G}(t^l,\uvec{u}(t^l))\psi^l(t)+O(\Delta t^{M+2}) \right]dt=\int_{t^0}^{t^m} \left[ O(\Delta t^{M+1})+O(\Delta t^{M+2}) \right]dt=$$
$$=O(\Delta t^{M+2})+O(\Delta t^{M+3})=O(\Delta t^{M+2}).$$
\end{proof}
Coming back to our initial goal, we will now define the operator $\lopdt^2$ directly from \ref{true} which we recall for clarity
$$\uvec{u}^m-\uvec{u}^0-\int_{t^0}^{t^m}\sum_{l=0}^{M} \uvec{G}(t^l,\uvec{u}^l)\psi^l(t)dt=\uvec{0} \quad \forall m=1,2,\dots,M$$ 
Let's first recast it in a clever way by noticing that the vectors $\uvec{G}(t^l,\uvec{u}^l)$ do not depend on time so we can put the finite sum out of the integral as well as these vectors (even if they are unknown because we don't know $\uvec{u}^l$). We thus get
$$\uvec{u}^m-\uvec{u}^0-\sum_{l=0}^{M} \uvec{G}(t^l,\uvec{u}^l)\int_{t^0}^{t^m}\psi^l(t)dt=\uvec{0} \quad \forall m=1,2,\dots,M$$ 
The Lagrangian polynomial functions $\psi^l(t)$ are known as well as the nodes $t^m$ so we can perform exactly the integrals and set
$$\int_{t^0}^{t^m}\psi^l(t)dt=\Delta t \int_{0}^{\frac{t^m-t^0}{\Delta t}}\psi^l\left(\Delta t s+t^0\right)ds=\Delta t\theta^m_l \quad \forall m=1,2,\dots,M \quad \forall l=0,1,\dots,M.$$
where
$$\theta^m_l=\int_{0}^{\frac{t^m-t^0}{\Delta t}}\psi^l\left(\Delta t s+t^0\right)ds.$$
Notice that we made a change of variable which transformed the interval $[t^0,t^m]$ into $[0,\frac{t^m-t^0}{\Delta t}]$ or equivalently $[t^0,t^M]$ into $[0,1]$. This allows us to work with normalized coefficients $\theta^m_l$ which depend just on the number and distribution of the nodes but not on $\Delta t$. 
\begin{observation}
In particular in the case of equispaced nodes we have
$$\theta^m_l=\int_{0}^{\frac{m}{M}}\hat{\psi}^l (s)ds \quad \forall m=1,2,\dots,M \quad \forall l=0,1,\dots,M$$
where $\hat{\psi}^l \quad l=0,1,\dots,M$ are the Lagrangian functions associated to the $M+1$ nodes $0<\frac{1}{M}<\frac{2}{M}<\dots<\frac{M-1}{M}<1.$
\end{observation}
So we have
$$\uvec{u}^m-\uvec{u}^0-\Delta t \sum_{l=0}^{M} \theta^m_l \uvec{G}(t^l,\uvec{u}^l)=\uvec{0} \quad \forall m=1,2,\dots,M$$ 
through which we finally get our our implicit, high-order, "tough" operator $\lopdt^2: \mathbb{R}^{(M \times N)} \rightarrow \mathbb{R}^{(M \times N)}$ defined as
\begin{equation}
\label{l2ODE}
\lopdt^2(\underline{\uvec{u}}) = \begin{pmatrix}
        \uvec{u}^M-\uvec{u}^0-\Delta t \sum_{l=0}^{M} \theta^M_l \uvec{G}(t^l,\uvec{u}^l)\\
        \vdots\\
        \uvec{u}^m-\uvec{u}^0-\Delta t \sum_{l=0}^{M} \theta^m_l \uvec{G}(t^l,\uvec{u}^l)\\
        \vdots\\
        \uvec{u}^1-\uvec{u}^0-\Delta t \sum_{l=0}^{M} \theta^1_l \uvec{G}(t^l,\uvec{u}^l)\\
\end{pmatrix}
\end{equation}
where 
$$\underline{\uvec{u}}=\left(
   \begin{array}{ccc}
   \uvec{u}^M\\
   \vdots\\
   \uvec{u}^m\\
   \vdots\\
   \uvec{u}^1
   \end{array}
\right).$$
We equip $X=Y=\mathbb{R}^{(M \times N)}$ with the infinity norm $\norm{\cdot}_\infty$.
Let's make some observations to make things clearer.
\begin{observation}
$M$ is the number of nodes $t^m$ in $[t_0,t_0+\Delta t]$ in which we do not know the solution to the ODEs system i.e. all the nodes apart from $t^0$ (in which we already know the exact solution $\uvec{u}_0$). Instead $N$ is the number of components of $\uvec{u}$ i.e. the number of equations of our ODEs system.
\end{observation}
\begin{observation}
It is very important to understand that in \ref{l2ODE} we are making a little change of notation. The vector $\underline{\uvec{u}}$ is the general argument of the operator $\lopdt^2$ and is obviously not in general its solution, it is just a not specified $M \times N$-dimesional vector.
We are now referring to the vector $\uvec{u}^m$ no more as the approximated solution in $t^m$ but as $N$ components of the general argument $\underline{\uvec{u}}$ of $\lopdt^2$. We get our approximation of the solution if we solve the operator i.e. if find $\usoldt$ s.t. $\lopdt^2(\usoldt)=\uvec{0}$.
If we have 
$$\usoldt=\left(
   \begin{array}{ccc}
   \uvec{u}^M_\Delta\\
   \vdots\\
   \uvec{u}^m_\Delta\\
   \vdots\\
   \uvec{u}^1_\Delta
   \end{array}
\right)$$ 
then we can say that $\uvec{u}^m_\Delta$ is our $M+1$-order accurate approximated solution in $t^m$ but in general $\uvec{u}^m$ is just a not specified $N$-dimensional real vector.
\end{observation}
\begin{observation}
The operator $\lopdt^2$ is implicit and $M+1$-order accurate. Its solution, i.e. $\usoldt$ s.t. $\lopdt^2(\usoldt)=\uvec{0}$, is made by all the solutions $\usol^m$ to \ref{true} which are  $M+1$-order accurate approximations of the solution of the ODEs system in the nodes $t^m$ $m=1,2,\dots,M$.
\end{observation}
\begin{observation}
\label{u0}
$\uvec{u}^0$ is not an unkonwn and for this reason it is not a argument of the operator $\lopdt^2$. It is "part" of the problem and it is "embedded" in the operator $\lopdt^2$.
\end{observation}
\begin{observation}
\label{obsnorm}
The space $X=Y=\mathbb{R}^{(M \times N)}$ is finite dimensional so all the norms that we can define on it are equivalent. Despite this, the choice of the norms is crucial and not trivial. This is due to the fact that the constants $\alpha_1$ and $\alpha_2$ must not depend on $\delta$. If we choose wrong norms we may have that the sub-coercivity of $\lopd^1$ is true but not the Lipschitz-continuity-like condition of $\lopd^1-\lopd^2$. In particular, as it will be soon clear, the first property is almost trivial to verify, while the second will need more attention to be proved. In general we have to take advantage of this fact by designing the norms $\norm{\cdot}_X$ and $\norm{\cdot}_Y$ in such a way that the inequality of the sub-coercivity property of $\lopd^1$ results in an equality. The basic idea is that we have to satisfy two "conflicting property": roughly speaking, the sub-coercivity property of $\lopd^1$ means that we need the norm $\norm{\cdot}_X$ to be "smaller" than the norm $\norm{\cdot}_X$ while vice versa the Lipschitz-continuity-like condition of $\lopd^1-\lopd^2$ means the exact opposite. Again, since the first property is easier to be verified we need to design the norms to relax it at most, this results essential for the second property to be verified. This is the the general guideline to follow in the designing of two norms satisfying the desired properties if we wish to apply the Deferred Correction to any context. Anyway this will be clearer when we will consider the the application to the balance laws.
\end{observation}
\begin{observation}
Let's remark that in principle we are not interested in all the components of $\lopdt^2$: we want an high-order approximation of the solution in the last node $t^M=t^0+\Delta t$. Despite this, the construction of the operator is strictly based on the approximation of the solution also in the other nodes. 
Let's also notice that the accuracy of each component of the solution of $\lopdt^2$ with respect to the exact solution to our initial problem \ref{ODE} in the corresponding node is $M+1$, not just for the component referred to the last node. Every component $\uvec{u}^m_\Delta$ of the solution $\usoldt$ is a $M+1$-order approximation of the solution in the correpsonding node $t^m$.
\end{observation}
\subsection{Derivation of $\lopdt^1$}
Also the operator $\lopdt^1$, just like $\lopdt^2$, will involve the approximations of the solution in the nodes $t^m$ $m=1,2,\dots,M$ and not just in the last one.
The "naive" operator $\lopdt^1$ is obtained by simply applying the Euler method to solve the system of ODEs. In particular let's refer again to the exact integration \ref{exint} that we recall for clarity
$$\uvec{u}(t^m)-\uvec{u}^0-\int_{t^0}^{t^m}\uvec{G}(t,\uvec{u}(t))dt=\uvec{0} \quad m=1,2,\dots,M.$$
If we apply the Euler method to get the approximate solution $\uvec{u}^m$ in the node $t^m$ we have
\begin{equation}
\label{Euler}
\uvec{u}^m-\uvec{u}^0-\Delta t \beta^m \uvec{G}(t^0,\uvec{u}^0)=\uvec{0}
\end{equation}
where $\beta^m=\frac{t^m-t^0}{\Delta t}$. Just like for $\theta^m_l$ in the context of the $\lopdt^2$ operator, we put in evidence $\Delta t$ to work with normalized coefficients.
The Euler method is well known to provide a first order approximation of the exact solution.
\begin{proposition}
The approximate solution $\uvec{u}^m$ got through the Euler method \ref{Euler} is first order accurate. 
\end{proposition} 
\begin{proof}
Again we consider the difference between the exact solution $\uvec{u}(t^m)$ to our ODEs system \ref{ODE} and $\uvec{u}^m$ got from \ref{Euler}. Through a first order Taylor expansion of $\uvec{u}(t)$ and from the fact that $\frac{d}{dt}\uvec{u}(t) = \uvec{G}(t,\uvec{u}(t))$ we have
$$\uvec{u}(t^m)-\uvec{u}^m=\uvec{u}^0+\uvec{G}(t^0,\uvec{u}^0)(t^m-t^0)+O(\Delta t^2)-\uvec{u}^0-\Delta t \beta^m \uvec{G}(t^0,\uvec{u}^0)=O(\Delta t^2)$$
because $\uvec{u}^0=\uvec{u}(t_0)=\uvec{u}_0$ and $\beta^m=\frac{t^m-t^0}{\Delta t}$.
\end{proof}
Directly from \ref{Euler} we get our explicit, low-order, "naive" operator
$\lopdt^2: \mathbb{R}^{(M \times N)} \rightarrow \mathbb{R}^{(M \times N)}$ defined as
\begin{equation}
\label{l1ODE}
\lopdt^1(\underline{\uvec{u}}) = \begin{pmatrix}

        \uvec{u}^M-\uvec{u}^0-\Delta t \beta^M \uvec{G}(t^0,\uvec{u}^0)\\
        \vdots\\
        \uvec{u}^m-\uvec{u}^0-\Delta t \beta^m \uvec{G}(t^0,\uvec{u}^0)\\
        \vdots\\
        \uvec{u}^1-\uvec{u}^0-\Delta t \beta^1 \uvec{G}(t^0,\uvec{u}^0)\\
        \end{pmatrix}
\end{equation}
where 
$$\underline{\uvec{u}}=\left(
   \begin{array}{ccc}
   \uvec{u}^M\\
   \vdots\\
   \uvec{u}^m\\
   \vdots\\
   \uvec{u}^1
   \end{array}
\right).$$
Let's remind that the norm chosen on $X=Y=\mathbb{R}^{(M \times N)}$ is the infinity norm $\norm{\cdot}_\infty$. We can make observations analogue to the ones that we made for the operator $\lopdt^2$.
\begin{observation}
In \ref{l1ODE} the vector $\underline{\uvec{u}}$ is the general argument of the operator $\lopdt^1$ and thus not in general its solution.
We are now referring to the vector $\uvec{u}^m$ as $N$ components of the general argument $\underline{\uvec{u}}$ of $\lopdt^1$ and not as the approximate solution in $t^m$ that we would get by solving the operator.
\end{observation}
\begin{observation}
The operator $\lopdt^1$ is explicit and easy to solve but just first-order accurate. Its solution is made by all the solutions to \ref{Euler} which are second-order accurate approximations of the solution of the ODEs system in the nodes $t^m$ $m=1,2,\dots,M$.
\end{observation}
\begin{observation}
\label{u0bis}
As always, $\uvec{u}^0$ is not an unkonwn and for this reason it is not a argument of the operator $\lopdt^1$, it is a known vector embedded in the operator.
\end{observation}
\begin{observation}
Also in this case let's remark that the solution to the operator $\lopdt^1$ would be first-order accurate in each of its components. 
\end{observation}



\section{From an operational perspective}
Before proving that the operators $\lopdt^1$ and $\lopdt^2$ fulfil the hypoteses required to apply the Deferred Correction let's make some practical considerations to give a clearer overview of the algorithm to implement. Assumed that the required properties hold, we can apply the method \ref{DeC} in this specific context and get
\begin{equation}
\label{DeCODE}
\lopd^1(\underline{\uvec{u}}^{(p)})=\lopd^1(\underline{\uvec{u}}^{(p-1)})-\lopd^2(\underline{\uvec{u}}^{(p-1)}) \quad p=1,2,\dots,P
\end{equation}
where
$$\underline{\uvec{u}}^{(p)}=\left(
   \begin{array}{ccc}
   \uvec{u}^{M,(p)}\\
   \vdots\\
   \uvec{u}^{m,(p)}\\
   \vdots\\
   \uvec{u}^{1,(p)}
   \end{array}
\right) \quad p=1,2,\dots,P$$
is the result of the $p$ iteration which is made by $M$ components $\uvec{u}^{m,(p)}$ corresponding to the approximate solutions in the subtimesteps $t^m$ $m=1,2,\dots,M$. Each one of them is itself a vector with $N$ components where $N$ is the number of equations of the system of ODEs \ref{ODE}.
In order to make things clearer let's look at the grid in figure \ref{grid}. On the ordinate axis we have the iterations while on the abscissa axis we have the subtimesteps.
\begin{figure}[hp] \centering{\includegraphics[scale=0.5]{grid.pdf}}
\caption{Grid of the iterations.}
\label{grid}
\end{figure}
The procedure \ref{DeCODE} results in an explicit iterative algorithm due to the fact that the operator $\lopdt^1$ is explicit. In fact the generic $p$ iteration reads
$$\begin{pmatrix}
    \vdots\\
        \uvec{u}^{m,(p)}-\uvec{u}^0-\Delta t \beta^m \uvec{G}(t^0,\uvec{u}^0)\\
        \vdots\\
  \end{pmatrix}=\begin{pmatrix}
        \vdots\\
        \uvec{u}^{m,(p-1)}-\uvec{u}^0-\Delta t \beta^m \uvec{G}(t^0,\uvec{u}^0)\\
        \vdots\\
  \end{pmatrix}+$$
  $$+\begin{pmatrix}
        \vdots\\
        \uvec{u}^{m,(p-1)}-\uvec{u}^0-\Delta t \sum_{l=0}^{M} \theta^m_l \uvec{G}(t^l,\uvec{u}^{l,(p-1)})\\
        \vdots\\
  \end{pmatrix}
$$       
and we can calculate $\underline{\uvec{u}}^{(p)}$ in an explicit way
$$\underline{\uvec{u}}^{(p)}=\left(
   \begin{array}{ccc}
   \vdots\\
   \uvec{u}^{m,(p)}\\
   \vdots\\
   \end{array}
\right)=\begin{pmatrix}
        \vdots\\
        \uvec{u}^0+\Delta t \sum_{l=0}^{M} \theta^m_l \uvec{G}(t^l,\uvec{u}^{l,(p-1)})\\
        \vdots\\
  \end{pmatrix}$$
\begin{observation}
We need a starting vector $\underline{\uvec{u}}^{(0)}$ for our iteration process. So we assume
$$\underline{\uvec{u}}^{(0)}=\left(
   \begin{array}{ccc}
   \uvec{u}^0\\   
   \vdots\\
   \uvec{u}^0\\
   \vdots\\
   \uvec{u}^0\\
   \end{array}\right)$$
i.e. we assume for every component $m=1,2,\dots,M$ of the argument, $\uvec{u}^{m,(0)}=\uvec{u}^0$.
\end{observation}
\begin{observation}
\label{compactness}
When we write $\uvec{u}^{0,(p)}$ in order to evaluate $\sum_{l=0}^{M} \theta^m_l \uvec{G}(t^l,\uvec{u}^{l,(p-1)})$ for $m=1,2,\dots,M$ we clearly mean $\uvec{u}^0$. As we already underlined in the observations \ref{u0} and \ref{u0bis}, the components $\uvec{u}^m$ of the global argument $\underline{\uvec{u}}^m$ of $\lopdt^1$ and $\lopdt^2$ correspond to the approximate solutions to the system of ODEs \ref{ODE} in the nodes $t^m$ $m=1,2,\dots,M$ where the solution is unkonwn. Instead in $t^0$ we have $\uvec{u}^0=\uvec{u}(t_0)=\uvec{u}_0$. The vector $\uvec{u}^0$ is fixed, it is part of the initial probem, it's not a variable. We should in principle write
$$\sum_{l=0}^{M} \theta^m_l \uvec{G}(t^l,\uvec{u}^{l,(p-1)})=\theta^m_0 \uvec{G}(t^0,\uvec{u}^0)+\sum_{l=1}^{M} \theta^m_l \uvec{G}(t^l,\uvec{u}^{l,(p-1)})$$
but to keep the notation more compact we avoid this and we set 
$$\uvec{u}^{0,(p)}=\uvec{u}^0 \quad p=0,1,\dots,P.$$
In practice we are making an abuse of notation: we are using the notation through which we refer in general to the variables $\uvec{u}^{m,(p)}$ to refer to the constant vector $\uvec{u}^0$ whenever $m$ or $p$ are equal to $0$.
\end{observation}
Now the algorithm is clear and we can focus on the optimal number of iterations to get the highest possible accuracy.

As already anticipated, at least in part, in observation \ref{optimaliterations} the Deferred Correction doesn't provide an approximation of the exact solution (in the nodes $t^m$ with $m=1,2,\dots,M$) to our ODEs system but an approximation of the solution to $\lopdt^2$ which is itself a $M+1$-order approximation of the exact solution. And it would be useless to make a "too high" number of iterations to approximate it with an accuracy order higher than $M+1$. To be clearer, if we define the vector
$$\underline{\uvec{u}}^{ex}=\left(
   \begin{array}{ccc}
   \uvec{u}^{ex}(t^M)\\
   \vdots\\
   \uvec{u}^{ex}(t^m)\\
   \vdots\\
   \uvec{u}^{ex}(t^1)
   \end{array}
\right)$$
made by the exact solutions to our ODEs system in the nodes $t^m$ $m=1,2,\dots,M$, the Deferred Correction doesn't approximate directly it but $\usoldt$ which is the solution to $\lopdt^2$ and it's a $M+1$-order accurate approximation of $\underline{\uvec{u}}^{ex}$ as we can see from the next proposition.
\begin{proposition}
\label{acc}
The vector $\underline{\uvec{u}}^{(P)}$ got from the final iteration $P$ of the Deferred Correction algorithm \ref{DeCODE} is a $P$-order approximation of $\usoldt$, the solution of the operator $\lopdt^2$.
\end{proposition}
\begin{proof}
From our first abstract accuracy estimate \ref{accuracyestimate} we have in this case
$$\norm{\underline{\uvec{u}}^{(P)}-\usoldt}_\infty \leq \left( \Delta \frac{\alpha_2}{\alpha_1} \right)^P\norm{\underline{\uvec{u}}^{(0)}-\usoldt}_\infty.$$
We just suffice to prove that $\norm{\underline{\uvec{u}}^{(0)}-\usoldt}_\infty$ is $O(\Delta t)$.
To do this let's consider $\usoldto$ solution to $\lopdt^1$ i.e. such that $\lopdt^1(\usoldto)=\uvec{0}$.
As we pointed out several times $\lopdt^1$ is a first-order accurate operator in the sense that its solution is first-order accurate with respect to the the vector $\uex$ of the exact solutions in the nodes $t^m$ $m=1,2,\dots,M$. Moreover it is explicit and very easy to solve. Directly from its definition \ref{l1ODE}, by solving $\lopdt^1(\usoldto)=\uvec{0}$, we have


$$\usoldto=\begin{pmatrix}
    \tilde{\uvec{u}}_\Delta^M\\
    \vdots\\
    \tilde{\uvec{u}}_\Delta^m\\
        \vdots\\
    \tilde{\uvec{u}}_\Delta^1
  \end{pmatrix}=\begin{pmatrix}

        \uvec{u}^0+\Delta t \beta^M \uvec{G}(t^0,\uvec{u}^0)\\
        \vdots\\
        \uvec{u}^0+\Delta t \beta^m \uvec{G}(t^0,\uvec{u}^0)\\
        \vdots\\
        \uvec{u}^0+\Delta t \beta^1 \uvec{G}(t^0,\uvec{u}^0)\\
        \end{pmatrix}.$$




Instead the operator $\lopdt^2$ is $M+1$-order accurate since its solution $\usoldt$ is $M+1$-order accurate with respect to $\uex$. Thus, by adding and subtracting $\uex$ and $\usoldto$ and by applying the triangular inequality we have
$$\norm{\underline{\uvec{u}}^{(0)}-\usoldt}_\infty \leq \norm{\underline{\uvec{u}}^{(0)}-\usoldto}_\infty+\norm{\usoldto-\uex}_\infty+\norm{\uex-\usoldt}_\infty=$$
$$=\norm{\underline{\uvec{u}}^{(0)}-\usoldto}_\infty+O(\Delta t^2)+O(\Delta t^{M+2}).$$
From a direct computation we have $\norm{\underline{\uvec{u}}^{(0)}-\usoldto}=O(\Delta t)$ in fact
$$\usoldto-\underline{\uvec{u}}^{(0)}=\begin{pmatrix}
  \uvec{u}^0+\Delta t \beta^M \uvec{G}(t^0,\uvec{u}^0)\\
  \vdots\\
  \uvec{u}^0+\Delta t \beta^m \uvec{G}(t^0,\uvec{u}^0)\\
  \vdots\\
  \uvec{u}^0+\Delta t \beta^1 \uvec{G}(t^0,\uvec{u}^0)
  \end{pmatrix}-\begin{pmatrix} 
  \uvec{u}^0\\   
   \vdots\\
   \uvec{u}^0\\
   \vdots\\
   \uvec{u}^0
   \end{pmatrix}=\Delta t \begin{pmatrix}
   \beta^M \uvec{G}(t^0,\uvec{u}^0)\\
  \vdots\\
  \beta^m \uvec{G}(t^0,\uvec{u}^0)\\
  \vdots\\
  \beta^1 \uvec{G}(t^0,\uvec{u}^0)
  \end{pmatrix}=O(\Delta t).$$
Remember that the coefficients $\beta^m$ are normalized coefficients that depend just on the distribution of the nodes $t^m$ but not on $\Delta t$ and $\uvec{G}(t^0,\uvec{u}^0)$ is a known constant vector.
It follows that $\norm{\underline{\uvec{u}}^{(P)}-\usoldt}_\infty=O(\Delta t^{P+1})$ which is the thesis.
\end{proof}
Thus we have that $\underline{\uvec{u}}^{(P)}$, resulting from the Deferred Correction procedure with $P$ iterations, is a $P$-order accurate approximation of the solution $\usoldt$ to $\lopdt^2$ which is itself a $M+1$-order approximation of the vector $\underline{\uvec{u}}^{ex}$ made by the exact solutions in the nodes $m=1,2,\dots,M$. From this we immediately see that the highest possible accuracy with respect to $\underline{\uvec{u}}^{ex}$ is bounded by $M+1$ and it is reached with a number of iterations at least equal to $M+1$.
\begin{proposition}
The accuracy of $\underline{\uvec{u}}^{(P)}$, resulting from the Deferred Correction procedure with $P$ iterations, with respect to the vector $\underline{\uvec{u}}^{ex}$ of the exact solutions in the nodes $m=1,2,\dots,M$ is $\min \left\lbrace P, M+1 \right\rbrace$.
\end{proposition}
\begin{proof}
Let's consider $\norm{\underline{\uvec{u}}^{(P)}-\underline{\uvec{u}}^{ex}}_\infty$. We add and subtract $\usoldt$, solution of $\lopdt^2$, and use the trianguar inequality, thus we have
$$\norm{\underline{\uvec{u}}^{(P)}-\underline{\uvec{u}}^{ex}}_\infty \leq \norm{\underline{\uvec{u}}^{(P)}-\usoldt}_\infty+\norm{\usoldt-\underline{\uvec{u}}^{ex}}_\infty.$$
As we repeated several times, since the operator $\lopdt^2$ is $M+1$ order accurate we have
$$\norm{\usoldt-\underline{\uvec{u}}^{ex}}_\infty=O(\Delta t^{M+2})$$
and since, due to the proposition \ref{acc}, $\underline{\uvec{u}}^{(P)}$ is  $P$-order approximation of $\usoldt$ it holds
$$\norm{\underline{\uvec{u}}^{(P)}-\usoldt}_\infty=O(\Delta t^{P+1}).$$
Thus we can write
$$\norm{\underline{\uvec{u}}^{(P)}-\underline{\uvec{u}}^{ex}}_\infty \leq O(\Delta t^{M+2})+O(\Delta t^{P+1})$$
which is the thesis.
\end{proof}
\begin{observation}
If we increase $P$ we increase the accuracy of $\underline{\uvec{u}}^{(P)}$ with respect to $\usoldt$ which is a $M+1$-order approximation of $\underline{\uvec{u}}^{ex}$. Since our aim is approximating $\underline{\uvec{u}}^{ex}$ and not $\usoldt$, increasing the number of iterations is useful only until we reach $M+1$-order accuracy. The optimal number of iterations is thus $P=M+1$, any other extra iteration would just be a waste of computational resources. So we set $P=M+1$, anyway to be more general, we will continue to use $P$ to refer to the number of iterations keeping in mind that the accuracy of $\underline{\uvec{u}}^{(P)}$ with respect to $\underline{\uvec{u}}^{ex}$ is $\min \left\lbrace P, M+1 \right\rbrace$ and that the most convenient number of iterations is $P=M+1$ which is the small number of iterations to provide $M+1$ accuracy.
\end{observation}
\begin{observation}
Despite the esteem on the accuracy of $\underline{\uvec{u}}^{(P)}$  being uniform with respect to all its components we are just interested in the component associated with the node $M$ becasuse our aim is to have an approximation of the solution to the system of ODEs \ref{ODE} in $t^M=t+\Delta t$. So at the end we take $\uvec{u}^{M,(P)}$ as our desired approximated solution at the time $t+\Delta t$.
\end{observation}
Now we can finally prove that the hypoteses on the operators $\lopdt^1$ and $\lopdt^2$ needed to apply the Deferred Correction method are fulfilled.
\section{Proof of the properties on $\lopdt^1$ and $\lopdt^2$}
Let's recall the hypoteses that are needed to apply the Deferred Correction method from the abstract formulation but characterizing them to our case
\begin{itemize}
\item[i)] \textbf{Existence of a solution to $\lopdt^2$} \\
$\exists ! \usoldt \in \mathbb{R}^{(M \times N)}$ solution of $\lopdt^2$ i.e. such that $\lopdt^2(\usoldt)=\uvec{0}$;
\item[ii)] \textbf{Sub-coercivity of $\lopdt^1$} \\
$\exists \alpha_1 \geq 0$ independent of $\Delta t$ s.t. $$\norm{\lopdt^1(\underline{\uvec{v}})-\lopdt^1(\underline{\uvec{w}})}_\infty \geq \alpha_1\norm{\underline{\uvec{v}}-\underline{\uvec{w}}}_\infty \quad \forall \underline{\uvec{v}},\underline{\uvec{w}} \in \mathbb{R}^{(M \times N)};$$
\item[iii)] \textbf{Lipschitz-continuity-like condition of $\lopdt^1-\lopdt^2$} \\
$\exists \alpha_2 \geq 0$ independent of $\Delta t$ s.t. $$\norm{\left[\lopdt^1(\underline{\uvec{v}})-\lopdt^2(\underline{\uvec{v}})\right]-\left[\lopdt^1(\underline{\uvec{w}})-\lopdt^2(\underline{\uvec{w}})\right]}_\infty \leq \alpha_2 \Delta t \norm{\underline{\uvec{v}}-\underline{\uvec{w}}}_\infty \quad \forall \underline{\uvec{v}},\underline{\uvec{w}}\in \mathbb{R}^{(M \times N)}.$$
\end{itemize}
The first property i.e. the exsistence of a unique solution to $\lopdt^2$ is assumed. Let's now consider two generic vectors $\underline{\uvec{v}},\underline{\uvec{w}}\in \mathbb{R}^{(M \times N)}$
$$\underline{\uvec{v}}=\left(
   \begin{array}{ccc}
   \uvec{v}^M\\
   \vdots\\
   \uvec{v}^m\\
   \vdots\\
   \uvec{v}^1
   \end{array}
\right) \quad
\underline{\uvec{w}}=\left(
   \begin{array}{ccc}
   \uvec{w}^M\\
   \vdots\\
   \uvec{w}^m\\
   \vdots\\
   \uvec{w}^1
   \end{array}
\right)$$
with $\uvec{v}^m$ and $\uvec{w}^m$ $m=1,2,\dots,M$ generic $N$-dimensional vectors.
and prove the other two conditions. As anticipated in the observation \ref{obsnorm}, the sub-coercivity of $\lopdt^1$ will be straightforward instead the Lipschitz-continuity-like condition of $\lopdt^1-\lopdt^2$ will be less trivial (even more delicate in the application of the Deferred Correction to the balance laws).
\subsection{Sub-coercivity of $\lopdt^1$}
From a direct computation we have
$$\lopdt^1(\underline{\uvec{v}})-\lopdt^1(\underline{\uvec{w}}) =             \begin{pmatrix}
        \uvec{v}^M-\uvec{u}^0-\Delta t \beta^M \uvec{G}(t^0,\uvec{u}^0)\\
        \vdots\\
        \uvec{v}^m-\uvec{u}^0-\Delta t \beta^m \uvec{G}(t^0,\uvec{u}^0)\\
        \vdots\\
        \uvec{v}^1-\uvec{u}^0-\Delta t \beta^1 \uvec{G}(t^0,\uvec{u}^0)\\
        \end{pmatrix}-\begin{pmatrix}
        \uvec{w}^M-\uvec{u}^0-\Delta t \beta^M \uvec{G}(t^0,\uvec{u}^0)\\
        \vdots\\
        \uvec{w}^m-\uvec{u}^0-\Delta t \beta^m \uvec{G}(t^0,\uvec{u}^0)\\
        \vdots\\
        \uvec{w}^1-\uvec{u}^0-\Delta t \beta^1 \uvec{G}(t^0,\uvec{u}^0)\\
        \end{pmatrix}=\begin{pmatrix}
        \uvec{v}^M-\uvec{w}^M\\
        \vdots\\
        \uvec{v}^m-\uvec{w}^m\\
        \vdots\\
        \uvec{v}^1-\uvec{w}^1
        \end{pmatrix}$$
so we have $\lopdt^1(\underline{\uvec{v}})-\lopdt^1(\underline{\uvec{w}}) =\underline{\uvec{v}}-\underline{\uvec{w}}$ and then
$$\norm{\lopdt^1(\underline{\uvec{v}})-\lopdt^1(\underline{\uvec{w}})}_\infty=\norm{\underline{\uvec{v}}-\underline{\uvec{w}}}_\infty$$
and thus the sub-coercivity of $\lopdt^1$ is verified and results in an equality.
Again we remark that $\uvec{u}^0$ is given, it's part of the problem and embedded in the operators $\lopdt^1$ and $\lopdt^2$.
\subsection{Lipschitz-continuity-like condition of $\lopdt^1-\lopdt^2$}
Again we consider a direct computation
$$\left[\lopdt^1(\underline{\uvec{v}})-\lopdt^2(\underline{\uvec{v}})\right]-\left[\lopdt^1(\underline{\uvec{w}})-\lopdt^2(\underline{\uvec{w}})\right]=$$
$$=\left[\begin{pmatrix} 
        \uvec{v}^M-\uvec{u}^0-\Delta t \beta^M \uvec{G}(t^0,\uvec{u}^0)\\
        \vdots\\
        \uvec{v}^m-\uvec{u}^0-\Delta t \beta^m \uvec{G}(t^0,\uvec{u}^0)\\
        \vdots\\
        \uvec{v}^1-\uvec{u}^0-\Delta t \beta^1 \uvec{G}(t^0,\uvec{u}^0)\\
        \end{pmatrix}-\begin{pmatrix}
        \uvec{v}^M-\uvec{u}^0-\Delta t \sum_{l=0}^{M} \theta^M_l \uvec{G}(t^l,\uvec{v}^l)\\
        \vdots\\
        \uvec{v}^m-\uvec{u}^0-\Delta t \sum_{l=0}^{M} \theta^m_l \uvec{G}(t^l,\uvec{v}^l)\\
        \vdots\\
        \uvec{v}^1-\uvec{u}^0-\Delta t \sum_{l=0}^{M} \theta^1_l \uvec{G}(t^l,\uvec{v}^l)\\
\end{pmatrix}\right]+$$
\begin{equation}
\label{intermezzo}
-\left[\begin{pmatrix}
        \uvec{w}^M-\uvec{u}^0-\Delta t \beta^M \uvec{G}(t^0,\uvec{u}^0)\\
        \vdots\\
        \uvec{w}^m-\uvec{u}^0-\Delta t \beta^m \uvec{G}(t^0,\uvec{u}^0)\\
        \vdots\\
        \uvec{w}^1-\uvec{u}^0-\Delta t \beta^1 \uvec{G}(t^0,\uvec{u}^0)\\
        \end{pmatrix}-\begin{pmatrix}
        \uvec{w}^M-\uvec{u}^0-\Delta t \sum_{l=0}^{M} \theta^M_l \uvec{G}(t^l,\uvec{w}^l)\\
        \vdots\\
        \uvec{w}^m-\uvec{u}^0-\Delta t \sum_{l=0}^{M} \theta^m_l \uvec{G}(t^l,\uvec{w}^l)\\
        \vdots\\
        \uvec{w}^1-\uvec{u}^0-\Delta t \sum_{l=0}^{M} \theta^1_l \uvec{G}(t^l,\uvec{w}^l)\\
\end{pmatrix}\right]
\end{equation}
where clearly $\uvec{v}^0=\uvec{w}^0=\uvec{u}^0$. As we pointed out several times expecially in the derivation of the operators $\lopdt^1$ and $\lopdt^2$, for example in the observations \ref{u0} and \ref{u0bis}, $\uvec{u}^0$ is not an unknown, it is a given vector, it is "part" of the problem and is embedded in the operators. We use $\uvec{v}^0$ and $\uvec{w}^0$ instead of $\uvec{u}^0$ for the sake of compactness as we did for $\uvec{u}^{0,(p)}$. The reader is referred to the observation \ref{compactness} for more clarity.
By elemntary algebra \ref{intermezzo} becomes
Again we consider a direct computation
$$\left[\lopdt^1(\underline{\uvec{v}})-\lopdt^2(\underline{\uvec{v}})\right]-\left[\lopdt^1(\underline{\uvec{w}})-\lopdt^2(\underline{\uvec{w}})\right]=\Delta t \begin{pmatrix} 
        \sum_{l=0}^{M} \theta^M_l \left[ \uvec{G}(t^l,\uvec{v}^l)-\uvec{G}(t^l,\uvec{w}^l) \right]\\
        \vdots\\
        \sum_{l=0}^{m} \theta^m_l \left[ \uvec{G}(t^l,\uvec{v}^l)-\uvec{G}(t^l,\uvec{w}^l) \right]\\
        \vdots\\
        \sum_{l=0}^{M} \theta^1_l \left[ \uvec{G}(t^l,\uvec{v}^l)-\uvec{G}(t^l,\uvec{w}^l) \right]
        \end{pmatrix}.$$
Remembering that $\theta^m_l$ $m=1,2,\dots,M$ $l=0,1,\dots,M$ are fixed normalized constant coefficients not depending on $\Delta t$ and that $\uvec{G}(t,\uvec{u})$ is Lipschitz-continuous with respect to $\uvec{u}$ uniformly with respect to $t$ with a Lipschitz constant $L$ we have
$$\norm{\left[\lopdt^1(\underline{\uvec{v}})-\lopdt^2(\underline{\uvec{v}})\right]-\left[\lopdt^1(\underline{\uvec{w}})-\lopdt^2(\underline{\uvec{w}})\right]}_\infty=\Delta t \norm{\begin{pmatrix} 
        \sum_{l=0}^{M} \theta^M_l \left[ \uvec{G}(t^l,\uvec{v}^l)-\uvec{G}(t^l,\uvec{w}^l) \right]\\
        \vdots\\
        \sum_{l=0}^{m} \theta^m_l \left[ \uvec{G}(t^l,\uvec{v}^l)-\uvec{G}(t^l,\uvec{w}^l) \right]\\
        \vdots\\
        \sum_{l=0}^{M} \theta^1_l \left[ \uvec{G}(t^l,\uvec{v}^l)-\uvec{G}(t^l,\uvec{w}^l) \right]
        \end{pmatrix}}_\infty \leq$$
        $$\leq \Delta t C \sum_{l=0}^{M} \norm{\begin{pmatrix} 
        \uvec{G}(t^l,\uvec{v}^l)-\uvec{G}(t^l,\uvec{w}^l)\\
        \vdots\\
        \uvec{G}(t^l,\uvec{v}^l)-\uvec{G}(t^l,\uvec{w}^l)\\
        \vdots\\
        \uvec{G}(t^l,\uvec{v}^l)-\uvec{G}(t^l,\uvec{w}^l)
        \end{pmatrix}}_\infty = \Delta t C \sum_{l=0}^{M} \norm{\uvec{G}(t^l,\uvec{v}^l)-\uvec{G}(t^l,\uvec{w}^l)}_{\infty, N} \leq$$ 
        $$\leq \Delta t C \sum_{l=0}^{M} L \norm{\uvec{v}^l-\uvec{w}^l}_{\infty, N} \leq \Delta t C L M\norm{\underline{\uvec{v}}-\underline{\uvec{w}}}_\infty$$
where $C$ is a positive constant depending on the coefficients $\theta^m_l$ and not on $\Delta t$. This proves the Lipschitz-continuity-like condition of $\lopdt^1-\lopdt^2$. For more clarity we underline that the infinity norm $\norm{\cdot}_{\infty,N}$ is applied to $N$-dimensional vectors (and not to $M\times N$-dimensional vectors like $\norm{\cdot}_\infty$).





\chapter{An introduction to balance laws}
In order to apply the Deferred Correction to some numerical methods for solving hyperbolic systems of balance laws, let's introduce the analytical problem. We would like to solve a general hyperbolic system of balance laws
%Too much theory in domains, I have to take omega domain and compact support in the closure
%Compact support in the closure otherwise we cannot embed DG in the weak problem
%Omega can also be unbounded because we want to emebed the case Omega=R or R^D which eliminates the BCs

%Last question? Is a weak fomrulation possible???
%No formal distributional formulation because the boundary and the initial conditions require to treat u as a function (even if it could be done maybe)
%So just naive weak formulation: multiplication by the test function and integral






\begin{equation}
\label{sys}
\frac{\partial}{\partial t}\uvec{u}(\uvec{x},t)+div_{\uvec{x}} \mathbf{F}(\uvec{u}(\uvec{x},t))=\mathbf{S}(\uvec{x},t,\uvec{u}(\uvec{x},t)) \quad (\uvec{x},t) \in \Omega\times \mathbb{R}^+_0
\end{equation}
with some suitable initial condition $\uvec{u}(\uvec{x},0)=\uvec{u}_0(\uvec{x})$ on $\Omega$ and boundary conditions on $\partial \Omega$.
We have that
\begin{itemize}
\item[•] $\Omega\subseteq \mathbb{R}^D$ is the spatial domain which is assumed to be a connected open set with dimension $D \in \mathbb{N}$ and its boundary $\partial \Omega$ is assumed to be smooth-enough; 

\item[•] $\uvec{u}:\overline{\Omega} \times \mathbb{R}^+_0 \rightarrow  \mathbb{R}^N$ is the unknown solution with $N \in \mathbb{N}$ number of scalar equations\footnote{Under a physical point of view, it can happen that not all the states of the space $\mathbb{R}^N$ are admissible for a solution $\uvec{u}$ to a system of balance laws. This limitation comes often with a physical explaination: for example if we consider the Euler equation we have that the density and the pressure have to be positive. This results in a restriction on the space of the possible values that can be assumed from the vector $\uvec{u}$ if we set physical initial and boundary conditions.};
\item[•] $\mathbf{F}:\mathbb{R}^N \rightarrow \mathbb{R}^N \times \mathbb{R}^D$ is the flux;
\item[•] $\mathbf{S}:\overline{\Omega} \times \mathbb{R}^+_0 \times \mathbb{R}^N \rightarrow \mathbb{R}^N \times \mathbb{R}^D$ is the source (or sink) term;
\item[•] $\uvec{u}_0:\overline{\Omega} \rightarrow \mathbb{R}^N$ is the known initial condition.
\end{itemize}
By $div_{\uvec{x}}\mathbf{F}(\uvec{u}(\uvec{x},t))$ we mean the divergence operator in the only spatial coordinates applied to the flux i.e. 
$$div_{\uvec{x}}\mathbf{F}(\uvec{u}(\uvec{x},t))=\sum_{d=1}^D \frac{\partial}{\partial x^d}\mathbf{F}_d(\uvec{u}(\uvec{x},t))$$ where $\mathbf{F}_d \in \mathbb{R}^N$ $d=1,2,\dots,D$ is the $d$ component of the flux $\uvec{F}=(\mathbf{F}_1,\mathbf{F}_2,\dots,\mathbf{F}_D)$.
We assume the flux to be Lipschitz-continuous and the source term to be continuous and Lipschitz-continuous with respect to $\uvec{u}$ uniformly with respect to $\uvec{x}$ and $t$.
For the sake of compactness in the following we will refer to $\uvec{u}(\uvec{x},t)$, $\mathbf{F}(\uvec{u}(\uvec{x},t))$ and $\mathbf{S}(\uvec{x},t,\uvec{u}(\uvec{x},t))$ simply by $\uvec{u}$, $\flux$ and $\source$.
\begin{observation}
To be more precise we also say that in order for the system \ref{sys} to be hyperbolic we need that any real linear combination of the Jacobians of the components of the flux calculated in any admittible $\uvec{u}$ must be real diagonalizable i.e. the matrix 
$$\sum_{d=1}^D \omega_d \frac{\partial\mathbf{F}_d}{\partial \uvec{u}}(\uvec{u})$$
must have $D$ real eigenvalues and $D$ real eigenvectors $\forall \uvec{u} \in \mathcal{U}$ and $\forall \uvec{\omega}=(\omega_1,\dots,\omega_D)^T \in \mathbb{R}^D$.
\end{observation}
\section{Derivation of balance laws}
We will now give a practical meaning to our analytical problem \ref{sys} by presenting its general derivation in the context of a generic real application.
There is a huge number of phenomena in physics, chemistry, biology, engineering and social sciences that can be modeled through a vectorial equation like the one in \ref{sys}. Basically whenever we want to express the conservation of some quantities, the components of the vector of the conserved quantities $\uvec{u}$, over a certain volume $\Omega$ (either physical or not) in a Eulerian formulation we end up with a hyperbolic system of balance laws. 
We start by an integral balance at a general time $t \in \mathbb{R}^+_0$ for a general control volume $V \subseteq  \Omega$ with respect to the quantities of interest $\uvec{u}$ as represented in figure \ref{conservation}. The set $V$ is an arbitrary smooth-enough connected open subset of $\Omega$.  
\begin{figure}[hp] \centering{\includegraphics[scale=0.5]{conservation.pdf}}
\caption{Representation of the conservation in the general control volume $V$.}
\label{conservation}
\end{figure}
The balance reads
\begin{equation}
\label{balance}
\int_V \frac{\partial}{\partial t}\uvec{u} d \uvec{x} + \int_{\partial V} \flux \cdot \uvec{\nu} d \sigma(\uvec{x}) = \int_V \source d \uvec{x} \quad  \forall V\subseteq \Omega, \forall t \in \mathbb{R}^+_0
\end{equation}
where $\uvec{\nu}$ is the outward-pointing normal vector to the surface $\partial V$. The equation \ref{balance} is known as the "integral" or "global" formulation of a system of balance laws. The first term represents the rate of change of the amount of conserved quantities $\uvec{u}$ in the control volume $V$. The second term represents the surface integral of the normal flux across $\partial V$ i.e. the amount of $\uvec{u}$ exiting from the control volume per unit of time. The last term represents the amount of conserved quantities $\uvec{u}$ generated by the source in the control volume per unit of time.
If we assume $\flux \cdot \uvec{\nu}$ to be regular in $\overline{V}$, we can apply the Gauss divergence theorem and get
$$\int_V \frac{\partial}{\partial t}\uvec{u} d \uvec{x} + \int_V div_{\uvec{x}}\flux d \uvec{x} = \int_V \source d \uvec{x}.$$


Since this balance holds for any time $t\in \mathbb{R}^+_0$ and for any smooth-enough control volume $V\subseteq \Omega $, under the assumption of regularity for the integrand functions, from the arbitrariness of $V$ we get the vectorial partial differential equation \ref{sys} that we recall for clarity
$$
\frac{\partial}{\partial t}\uvec{u}+div_{\uvec{x}} \mathbf{F}=\mathbf{S} \quad \forall (\uvec{x},t) \in \Omega\times \mathbb{R}^+_0
$$
which is referred as the "differential" or "local" formulation of the system of balance laws.
\begin{observation}
The control volume $V$, as well as the spatial domain $\Omega$, can either be fixed or depend on time. Clearly in the first case the normal $\nu$ just depends on space while in the second case it also depends on time. Let's consider for example a deformable body with its shape varying in time. In this case the spatial domain changes in time. Anyway we will just consider spatial domains fixed in time.
\end{observation}
\begin{observation}
So far the spatial domain $\Omega$ can also be unbounded. Clearly when we will deal with the problem under a numerical point of view we will be interested in bounded spatial domains and we will have to set a final time $T$.¨ 
\end{observation}
Several boundary conditions can be imposed. In many applications they are a representation of the physical conditions that we want to reproduce. In these cases we can derive them from the physical model. Due to the multiplicity of the existing types of boundary conditions we do not treat their derivation in detail. 

%However we present a particular class of boundary conditions named "inflow-outflow" which will be the reference ones in our numerical context.
%\subsection{Inflow-outflow boundary condition}
%The inflow-outflow boundary condition consists in imposing
%$$ \min{\left\lbrace\frac{\partial\mathbf{F}}{\partial \uvec{u}}(\uvec{u}(\uvec{x}))\cdot \uvec{\nu}(\uvec{x}) \cdot (\uvec{u}(\uvec{x})-\uvec{u}_b(\uvec{x})) ,\uvec{0}\right\rbrace }=\uvec{0} \quad \forall \uvec{x} \in \partial{\Omega} $$
%where $\uvec{u}_b:\partial{\Omega}\rightarrow \mathbb{R}^N$ is a given function and the minimum is meant component by component.
%This corresponds to
































\section{Weak solutions}
We will now make a small description of the solutions that we expect to a system like \ref{sys}.
A "classical" or "strong" solution to \ref{sys} is a smooth-enough vectorial function satisfying it pointwise (and obviously satisfying also the initial and the bounndary conditions).
It is well known that the notion of "classical" solution is not "sufficient": for nonlinear fluxes we can get discontinuities in the solution even if we start from smooth initial conditions. In the general case there is no hope to find a strong solution.
Since the mentioned discontinuities show up also in the phenomena that we want to model\footnote{Let's consider for example the shock waves generated by an aircraft moving at supersonic speed or discontinuities in the properties of non-homogeneous media.} this is not a negligible problem and we need to solve it. Let's notice also that the discontinuities in the solution determine a violation of the hypoteses of regularity under which we passed from the integral formulation to the differential one.
To deal with this problem we introduce the weak solutions. We look at our equation \ref{sys} under a distributional point of view. 

%%%A non-naive weak formulation is not possible: if we want the boundary condition to figure in the weak problem we can just multiply by a test function and integrate.
%%%We have the boundary figuring (specially in time) and we cannot evaluate an abstract operator like a distribution pointwise... There is to say that we know the value at the initial time.

In practice we multiply our equation by a scalar smooth arbitrary test function with compact support and integrate in space and time

$$\int_{\Omega \times \mathbb{R}^+_0} \left(\frac{\partial}{\partial t}\uvec{u}+div_{\uvec{x}} \flux\right)\varphi(\uvec{x},t) d\uvec{x} dt=\int_{\Omega \times \mathbb{R}^+_0} \source\varphi(\uvec{x},t) d\uvec{x} dt \quad \forall \varphi \in C^1_o(\overline{\Omega} \times \mathbb{R}^+_0).$$
Under hypoteses of regularity of the solution, the flux and the source term we can use the divergence theorem and write
$$\int_{\Omega} \left( \left[\uvec{u}\varphi(\uvec{x},t)\right]_{t=0}^{t=+\infty} - \int_{\mathbb{R}^+_0}\uvec{u}\frac{\partial}{\partial t}\varphi(\uvec{x},t) dt\right) d\uvec{x}+\int_{\mathbb{R}^+_0}\left( \int_{\partial \Omega} \varphi(\uvec{x},t)\flux\cdot\uvec{\nu}d\sigma(\uvec{x})- \int_{\Omega} \flux \cdot \nabla_{\uvec{x}} \varphi(\uvec{x},t) d\uvec{x}\right)d t=$$
$$\int_{\Omega \times \mathbb{R}^+_0} \source\varphi(\uvec{x},t) d\uvec{x} dt \quad \forall \varphi \in C^1_o(\overline{\Omega} \times \mathbb{R}^+_0)$$
where $\nabla_{\uvec{x}}$ is the gradient in the only spatial coordinates.
Thus due to the compact support of $\varphi$ we have
$$-\int_{\Omega}\uvec{u}(\uvec{x},0 ) \varphi(\uvec{x},0)d \uvec{x}-\int_{\Omega \times \mathbb{R}^+_0}\left(\uvec{u}\frac{\partial}{\partial t} \varphi(\uvec{x},t) + \flux \cdot \nabla_{\uvec{x}} \varphi(\uvec{x},t) \right) d\uvec{x}dt+$$
\begin{equation}
\label{weak}
+ \int_{\partial\Omega \times \mathbb{R}^+_0}\left( \varphi(\uvec{x},t)\flux\cdot\uvec{\nu} \right) d\sigma(\uvec{x})dt=\int_{\Omega \times \mathbb{R}^+_0} \source\varphi(\uvec{x},t) d\uvec{x} dt \quad \forall \varphi \in C^1_o(\overline{\Omega} \times \mathbb{R}^+_0).
\end{equation}
Equation \ref{weak} is referred as the weak formulation of the system of balance laws \ref{sys}.
\begin{observation}
In reality the integral over the surface $\partial \Omega \times \mathbb{R}^+_0$ (and so the boundary conditions) would require more rigorous specifications under an analytic point of view. More in general we admit that a more formal position of the problem is possible in the context of the distributions i.e. linear and continuous (w.r.t. a certain notion of convergence) operators over a space of test functions but it is out of the purpose of this document.

%%%%%%%%%%REFERENCE TO ERN and TO ALBERT

\end{observation}
A weak solution is a function $\uvec{u}:\overline{\Omega} \times \mathbb{R}^+_0 \rightarrow \mathbb{R}^N$ satisfying the integral relation \ref{weak} for any arbitrary test function $\varphi \in C^1_o(\overline{\Omega} \times \mathbb{R}^+_0)$.
To a certain extent we are going back to an integral formulation like the one presented in equation \ref{balance} but slightly different. We actually got the differential formulation \ref{sys} from the integral one \ref{balance} under some regularity assumptions which, as we said, are not often reliable. A return to an integral formulation is abundantly justified. Moreover the weak formulation require less regularity from a possible solution. If we look at \ref{weak}, all the derivatives are now on the smooth test function, even a discontinuous function can be a weak solution. 
\begin{observation}
We can simply verify that a strong solution is also a weak solution. Clearly the viceversa is not true since the weak solutions can have discontinuities. Moreover it is also easy to see that if a weak solution is smooth in a neighborhood of an arbitrary point $(\uvec{x},t)$ of the space-time domain then it satisfies in the strong sense the initial equation in that point.
\end{observation}
\begin{observation}
If we take $\Omega=\mathbb{R}^D$, due to the compact support of $\varphi$, we have that \ref{weak} reduces to 
$$-\int_{\mathbb{R}^D}\uvec{u}(\uvec{x},0 ) \varphi(\uvec{x},0)d \uvec{x}-\int_{\mathbb{R}^D \times \mathbb{R}^+_0}\left(\uvec{u}\frac{\partial}{\partial t} \varphi(\uvec{x},t) + \flux \cdot \nabla_{\uvec{x}} \varphi(\uvec{x},t) \right) d\uvec{x}dt=$$
$$=\int_{\mathbb{R}^D \times \mathbb{R}^+_0} \source\varphi(\uvec{x},t) d\uvec{x} dt \quad \forall \varphi \in C^1_o(\mathbb{R}^D \times \mathbb{R}^+_0).$$
\end{observation}

Many other things can be said about weak solutions: it can be shown that a shock (a discontinuity of first kind) in a weak solution must fulfil the Rankine-Hugoniot condition which represents a relation between the "jump" and the "speed"; the weak solutions are in general not unique so we need a criterion to select the ones which have a physical meaning and this is often given by some inequalities, the weak solutions satisfying these inequalities are said "entropy solutions". 
A deeper analysis is out of the purpose of this document. 
What we have said so far can seem to be very distant from our initial goal of showing how to apply the Deferred Correction to the balance laws but in the author's opinion it was fundamental in order to give a clear idea of the problem that we are trying to solve. Moreover almost all the numerical approaches are designed to detect approximations of  weak (eventually entropic) solutions.

We can now enter into the details of the numerical methods for hyperbolic systems of balance laws.

\chapter[Semidiscrete formulation and mass matrix]{The semidiscrete formulation and the role of the mass matrix}
Before presenting the Residual Distribution and Discontinuous Galerkin Finite Elements methods\footnote{We will just present the methods without treating the problem of the convergence of the detected approximated solutions to the desired analytical weak ones.} and show how to apply the Residual Distribution in the context of these ones, we introduce from a general point of view the semidiscrete formulation which is a general strategy shared by several numerical methods for hyperbolic systems of balance laws (among which Residual Distribution and Discontinuous Galerkin Finite Elements) to deal separately with the space and time discretizations. It leads to a system of ODEs commonly involving a mass matrix. We will focus on the problem of the inversion of this matrix.\\
We would like to solve numerically an hyperbolic system of balance laws in the general form \ref{sys} that we recall for clarity
$$
\frac{\partial}{\partial t}\uvec{u}+div_{\uvec{x}} \mathbf{F}=\mathbf{S} \quad \forall (\uvec{x},t) \in \Omega\times \mathbb{R}^+_0.
$$
As anticipated, from a numerical point of view we are interested in bounded spatial domains $\Omega$ and we have to choose a final time $T\in \mathbb{R}^+$.
Thus we would like to find $\uapp$, approximation in $\overline{\Omega} \times [0,T]$ of a weak (eventually entropic) solution $\uvec{u}$ to our problem i.e. satisfying the integral relation \ref{weak} with $\Omega\subseteq \mathbb{R}^D$ bounded connected open set with a smooth-enough boundary (and eventually some entropy inequalities).
\begin{observation}
Often the reference spatial domain for the numerical method is just an approximation of the analytical spatial domain $\Omega$ in which we would like to solve the initial equation. This reference domain is usually referred as "discretized". Here we assume for simplicity that the discretized domain coincides with the analytical one. So we use indifferently $\Omega$ for the "analytical" and "numerical" reference domain.
\end{observation}
A common approach to solve numerically a system of balance laws consists in "splitting" the resolution of the equation in space and the resolution of the equation in time. We assume at any time $t \in [0,T]$ the approximated solution $\uapp$ in a finite dimensional space $(V_h)^N$ where $V_h=span\lbrace \varphi_i\rbrace_{i=1,2,\dots,I}$ and the (known) functions $\varphi_i:\overline{\Omega}\rightarrow \mathbb{R}$ are a basis of $V_h$ and depend just on space. 
\begin{observation}
The functions $\varphi_i$ are usually but not mandatorily piecewise polynomial functions. Each one of them corresponds usually to a node $\uvec{x}_i$ located somewhere in the closure of the discretized spatial domain. In these cases we have that the value of the basis function $\varphi_i$ in its associated node $\uvec{x}_i$ is not $0$ i.e. $\varphi_i(\uvec{x}_i)\neq0$. The nodes $\uvec{x}_i$ $i=1,2,\dots,I$ are often referred as the degrees of freedom, DoFs, of the approximated solution. 
\end{observation}
In other words we require every component of the approximated solution at any time $t$ to be in the space of functions $V_h$.
In practice we are assuming 
\begin{equation}
\label{discr}
\uapp(\uvec{x},t)=\sum_{i=1}^I\uvec{c}_i(t)\varphi_i(\uvec{x}) \quad \forall(\uvec{x},t)\in \overline{\Omega}\times [0,T]
\end{equation} 
with $\uvec{c}_i(t) \in \mathbb{R}^N \quad \forall i=1,2,\dots, I \quad \forall t\in [0,T]$.
\begin{observation}
The basis functions $\varphi_i$ are fixed and known, the only unknowns are the coefficients $\uvec{c}_i(t)$. Looking for $\uapp$ is equivalent to look for $\uvec{c}_i(t) \quad i=1,2,\dots, I \quad t\in [0,T]$.
\end{observation}
The problem is now to evaluate these coefficients in $[0,T]$.
Thus, according to some mathematical and physical criteria, some relations based on the initial equation \ref{sys} are derived. From the combination of these relations and the discretization \ref{discr} we usually get a system of ordinary differential equations involving a mass matrix $\massmatrix$ which reads
\begin{equation}
\label{semidiscr}
\massmatrix \frac{d}{dt}\uvec{c}(t)=\uvec{H}(t,\uvec{c}(t))
\end{equation} 
where the unknown vector $\uvec{c}(t)$ has $I\times N$ dimensions and contains all the $N$-dimensional unkonwn vectors $\uvec{c}_i(t)$ as components
$$\uvec{c}(t)=\begin{pmatrix}
\uvec{c}_1(t)\\
\vdots\\
\uvec{c}_i(t)\\
\vdots\\
\uvec{c}_I(t)\\
\end{pmatrix}\in \mathbb{R}^{I\times N}.$$
We have obviously $\massmatrix \in \mathbb{R}^{(I\times N)\times(I\times N)}$ and $\uvec{H}(t,\uvec{c}(t))\in \mathbb{R}^{I\times N}$ $\forall t \in [0,T].$  
The system of ODEs \ref{semidiscr} is usually referred as "semidiscrete formulation" of the particular method that we are using and we have to solve it in time.
The initial condition $\uvec{c}(0)=\uvec{c}_0$ is given by the analytical initial condition on the hyperbolic system of balance laws \ref{sys} that we want to solve (projected into the finite dimensional space $(V_h)^N$).

\begin{observation}
The Discontinuous Galerkin and the Residual Distribution methods are based on this approach, so in both cases we get a coupled system of ODEs like the one in equation \ref{semidiscr}.
In the Discontinuous Galerkin case the mass matrix is not dependent on time nor on $\uvec{c}(t)$, moreover it is nonsingular, block diagonal and defined positive. Usually in this case the mass matrix is inverted and the resulting system of ODEs
$$ \frac{d}{dt}\uvec{c}(t)=\massmatrix^{-1}\uvec{H}(t,\uvec{c}(t))$$
is solved with a classical ODE solver, typically a Runge-Kutta scheme\footnote{To be more specific we also say that a huge variety of ODEs solvers exists and depending on the features we are interested in we can choose particular methods rather than others. For example if we are interested in controlling the total variation of the solution, we can choose a strong stability preserving Runge-Kutta method (SSP-RK). If we are interested in the preservation of the non-negativity of certain variables we can choose a positivity preserving scheme. We do not enter into the detail of such methods.}.
In the general case the matrix $\massmatrix$ depends on the time $t$ and on the vector $\uvec{c}(t)$ and we don't even know whether it is invertible or not. This is the case for example of the Residual Distribution methods. In such occurrences trying to solve the system as in the Discontinuous Galerkin case would lead to very big problems\footnote{As we said, we have no information about the invertibility of the matrix. Moreover since it depends on $t$ and on $\uvec{c}(t)$, a classical approach would be very likely to require to recompute it and invert it (assuming that this is possible) at each time step.} thus avoiding the inversion of the mass matrix is crucial. Here the Deferred Correction comes to aid.
\end{observation}
\begin{observation}
The Finite Volume methods are characterized by a different approach. The coefficients represent the averages of the approximated solution in some regions of the (discretized) space domain but also in this case we arrive to a semidiscrete formulation like equation \ref{semidiscr}. In this case the mass-matrix is diagonal nonsingular and its entries are the measures of the mentioned regions. Thus in this case the inversion of the mass-matrix is straightforward.
\end{observation}
\begin{observation}
%FV -> polynomials of order 0
%Euler first order -> order 1 accuracy
%=> Order 1 accuracy
%FV -> Linear reconstruction = polynomials of order 1
%second order RK -> order 2 accuracy
%=> Order 2 accuracy
%But not always, for example in SUPG and more in genreal in RD schemes Galerkin +diffusion with a stabilization term not consistent
Generally a spatial discretization made with polynomial functions $\varphi_i$ of order $R$ and an ODE solver $R+1$-order accurate lead to a globally $R+1$-order accurate scheme. This means that the coefficients $\tilde{\uvec{c}}(t^{*})$ that characterize the approximated solution $\uvec{u}_h$ at the time $t^{*}$ determined through the used method are $R+1$-order accurate approximations of the coefficients $\uvec{c}(t^{*})$ that we obtain projecting the exact solution at the time $t^{*}$ over $(V_h)^N$.
But let's underline that a spatial discretization made with polynomial functions of order $R$ and an ODE solver $R+1$-order accurate are just necessary but not sufficient ingredients for $R+1$-order accuracy. If the equations of the system \ref{semidiscr} are not carefully derived, the accuracy is wasted and we get unexpected low orders of convergence.
\end{observation}
Let's now present more in detail the Residual Distribution and the Discontinuous Galerkin Finite Elements methods and see how to apply the Deferred Correction to get explicit arbitrary high-order accurate schemes for hyperbolic systems of balance laws.
\chapter{The Residual Distribution method}
In order to present the Residual Distribution method we refer again to our initial problem.
We would like to find an approximation $\uapp$ in $\overline{\Omega} \times [0,T]$ of a weak (eventually entropic) solution of an hyperbolic system of balance laws whose general form is given by equation \ref{sys} which is recalled here
$$
\frac{\partial}{\partial t}\uvec{u}+div_{\uvec{x}} \mathbf{F}=\mathbf{S} \quad \forall (\uvec{x},t) \in \Omega\times \mathbb{R}^+_0.
$$
Again we remark that now $\Omega$ is a bounded domain with smooth-enough boundary $\partial \Omega$.
Some suitable initial and boundary conditions are assumed.
We consider a tessellation $\tess$ of the closure of the spatial domain i.e. a finite family of $D$-dimensional nonoverlapping polytopal subsets $K$ of $\overline{\Omega}$ which cover it exactly\footnote{Again we remind the assumption that the analytical and the discretized spatial domain coincide. The elements of the tessellation cover $\overline{\Omega}$ exactly without any approximation.} with characteristic length $h$, i.e. such that
\begin{itemize}
\item[•] $\mathring{K} \neq \emptyset \quad \forall K \in \tess$
\item[•] $\mathring{K_i} \cap \mathring{K_j}=\emptyset \quad \forall K_i, K_j \in \tess \quad s.t. \quad K_i \neq K_j;$
\item[•] $\cup_{K \in \tess} K=\overline{\Omega}.$
\item[•] $\sup_{K \in \tess} diam(K)=h$ where $diam(K)=\sup_{\uvec{x},\uvec{y} \in K}\norm{\uvec{x}-\uvec{y}}_2$
\end{itemize}
\begin{observation}
The shape of the sets of the tessellation cannot be chosen arbitrarily, there are some regularity requirements that must be satisfied but we do not treat them in detail.
%Reference to quarteroni
\end{observation}
The tessellation $\tess$ is often called "triangulation" and its elements are ususally referred as "cells".
Usually but not mandatorily the polytopes of the tessellation are simplices. In figure \ref{simplices} are represented the simplices in one, two and three dimensions.

\begin{figure}[hp] \centering{\includegraphics[scale=0.5]{simplices.pdf}}
\caption{Simplices in one, two and three dimensions.}
\label{simplices}
\end{figure}

Usually but not mandatorily the tessellation is "conformal". Roughly speaking it means that each $D-1$-dimensional polytopal face $f$ belongs at most to two polytopes.
For example, for $D=3$ this is equivalent to say that two different elements of the tessellation can share at most one vertex or a whole edge or a whole face while for $D=2$ they can share at most one vertex or a whole edge (i.e. a $1$-dimensional face). To be clearer have a look at the picture \ref{tessellations} in which a conformal and a non-conformal bidimensional tessellations are shown.

\begin{figure}[hp]
\begin{subfigure}{.8\textwidth}
  \centering
  % include first image
  \includegraphics[width=.8\linewidth]{conformal.pdf}  
  \caption{Conformal tessellation.}
  \label{conformal}
\end{subfigure}
\begin{subfigure}{.8\textwidth}
  \centering
  % include second image
  \includegraphics[width=.8\linewidth]{conformalnon.pdf}  
  \caption{Non conformal tessellation.}
  \label{conformalnon}
\end{subfigure}
\caption{Example of tessellations of a bidimensional rectangular domain.}
\label{tessellations}
\end{figure}

We will refer to a conformal tessellation made by simplices but our description will be in general valid also for more general tessellations.
To get the semidiscrete formulation we assume that the approximated solution $\uapp$ is at any time $t\in [0,T]$ in the space $(V_h)^N$ where $V_h$ is a space of continuous piecewise-polynomial functions defined as
$$V_h=\lbrace g \in C^0(\overline{\Omega}) \quad s.t. \quad g\vert_K \in \mathbb{P}_M \quad \forall K \in \tess\rbrace.$$
To this end we choose a basis $\lbrace \varphi_i \rbrace_{i=1,2,\dots,I}$ of $V_h$, for example the Lagrange polynomials or the Bernstein polynomials, and we assume the discretization \ref{discr} that is recalled here
$$\uapp(\uvec{x},t)=\sum_{i=1}^I\uvec{c}_i(t)\varphi_i(\uvec{x}) \quad \forall(\uvec{x},t)\in \overline{\Omega}\times [0,T]$$
with $\uvec{c}_i(t)$ $N$-dimensional coefficients dedpending on time.
The general basis function $\varphi_i$ is associated to a node $\uvec{x}_i\in\overline{\Omega}$ and is such that $\varphi_i(\uvec{x}_i) \neq 0$. These $I$ nodes associated to the $I$ basis functions are the degrees of freedom of the solution and belong in general to one or more elements of the tessellation $\tess$. Each basis function $\varphi_i$ has its support contained in the union of the elements containing the node to which it is associated.
In other words if $\varphi_i$ is associated to the node $\uvec{x}_i\in \overline{\Omega}$ and we denote with $K_i$ the set of the elements of the tessellation containing the node $\uvec{x}_i$ i.e.
$$K_i=\lbrace K\in \tess \quad s.t. \quad \uvec{x}_i\in K \rbrace$$ 
then $\varphi_i \in C^0_o(\cup_{K \in K_i} K)$.
The situation is represented in figure \ref{DoF}: three nodes are put in evidence and coloured respectively in green, yellow and red as well as the elements of the associated $K_i$ i.e. the elements containing them which are coloured with the same colours but in a lighter shade. 

\begin{figure}[hp] \centering{\includegraphics[scale=0.6]{DoF3.pdf}}
\caption{Some degrees of freedom and the associated $K_i$.}
\label{DoF}
\end{figure}

For example, the basis function associated to the red degree of freedom has its support contained in the union of the six red elements of the tessellation which contain it.
The basis function associated to the yellow node is non-zero just in the union of the two yellow elements containing it.
And obviosly the basis function associated to the green degree of freedom has support contained in the only (green) element to which the green node belongs.
Moreover in the particular case of the Residual Distribution approach we always normalize the basis functions s.t. their sum is identically equal to $1$ over $\overline{\Omega}$ i.e.
\begin{equation}
\label{normalization}
\sum_{i=1}^I\varphi_i(\uvec{x}) \equiv 1 \quad \forall \uvec{x} \in \overline{\Omega}.
\end{equation}
\begin{observation}
This is usually but not mandatorily done also for other methods.
\end{observation}
\begin{observation}
The Lagrangian basis functions are characterized by the fact that their value is $1$ in the node to which they are associated and $0$ in all the other nodes i.e.
$$\varphi_i(\uvec{x}_j)=\delta_{ij} \quad \forall i,j=1,2,\cdots,I$$
where $\delta_{ij}$ is the Kronecker delta.
Thus in this case the coefficients $\uvec{c}_j(t)$ of the discretization \ref{discr} represent directly the value of the components of the approximated solution $\uapp$ in the node $\uvec{x}_j$ at the time $t$
$$\uapp(\uvec{x}_j,t)=\sum_{i=1}^I\uvec{c}_i(t)\varphi_i(\uvec{x}_j)=\sum_{i=1}^I\uvec{c}_i(t)\delta_{ij}=\uvec{c}_j(t) \quad \forall t \in [0,T].$$
This is in general not true for other choices of the basis, for example for the Bernstein polynomials.
\end{observation}
Now we have all the elements that we need to go into the details of the Residual Distribution approach.


%RD

Basically the Residual Distribution can be summarised into three steps:
\begin{itemize}
\item[i)] \textbf{Definition of the element residuals}\\
For each element $K$ of the tessellation $\tess$ we define the element residual $\elres$ which is what we get if we make the integral in space of the equation \ref{sys} over the element $K$ and apply the divergence theorem
$$\forall K \in \tess \quad \elres=\int_K 
\frac{\partial}{\partial t}\uapp(\uvec{x},t)d\uvec{x}+\int_{\partial{K}} \mathbf{F}(\uapp(\uvec{x},t))\cdot \nu d \sigma(\uvec{x})+$$
\begin{equation}
\label{elementres}
-\int_K \mathbf{S}(\uvec{x},t,\uapp(\uvec{x},t)) d\uvec{x}  \quad t \in [0,T];
\end{equation}
\item[ii)] \textbf{Definition of the node residuals}\\
For each element $K$ we consider the degrees of freedom belonging to it, $\uvec{x}_i \in K$, and define the node residuals $\noderes$ such that they satisfy a specific conservation relation
$$\forall K \in \tess \quad \forall \uvec{x}_i \in K \quad \noderes \quad t \in [0,T] \quad s.t.$$
\begin{equation}
\label{conservation}
\sum_{\uvec{x}_i \in K} \noderes=\elres \quad \forall K \in \tess \quad \forall t  \in [0,T];
\end{equation}
\item[iii)] \textbf{Imposition of the balance at the nodes}\\
For each degree of freedom $\uvec{x}_i$ we impose an equilibrium between all the node residuals $\noderes$ of the elements that contain that degree of freedom 
\begin{equation}
\label{nodebalance}
\sum_{K \in K_i} \noderes=\uvec{0} \quad t \in [0,T] \quad \forall \i=1,2,\dots,I
\end{equation}
where we recall that $K_i$ is the set of the elements of the tessellation containing the node $\uvec{x}_i$. The balance \ref{nodebalance} represents the semidiscrete formulation of the Residual Distribution method, a system of nonlinear ODEs that we have to solve in time.
\end{itemize}
We will now give a physical interpretation of the Residual Distribution approach. Let's go back to the steps of the method and look at them under a new perspective.
\begin{itemize}
\item[i)] \textbf{Balances at the elements}\\
We start by an initial balance at each element. We consider our initial equation \ref{sys} and we move the source at the left hand side so to have
$$
\frac{\partial}{\partial t}\uvec{u}(\uvec{x},t)+div_{\uvec{x}} \mathbf{F}(\uvec{u}(\uvec{x},t))-\mathbf{S}(\uvec{x},t,\uvec{u}(\uvec{x},t))=\uvec{0}.
$$
If we integrate this quantity over each element $K$ of the tessellation at a generic time $t$ and we apply the divergence theorem we get the integral balance
$$
\int_K \frac{\partial}{\partial t}\uvec{u}(\uvec{x},t)d\uvec{x}+\int_{\partial{K}} \mathbf{F}(\uvec{u}(\uvec{x},t))\cdot\nu d\sigma(\uvec{x})-\int_K \mathbf{S}(\uvec{x},t,\uvec{u}(\uvec{x},t))d\uvec{x}=\uvec{0} \quad \forall t \in \mathbb{R}^+_0.
$$
This is the meaning of the element residuals $\elres$ even if obviously to evaluate them in our numerical method we consider the approximated solution $\uapp(\uvec{x},t)=\sum_{i=1}^I\uvec{c}_i(t)\varphi_i(\uvec{x})$ in place of the exact solution $\uvec{u}$.
\item[ii)] \textbf{Isolation of the contributions of the nodes to the element balance}\footnote{This step is sometimes referred as the "splitting" of the element residual into the node residuals. Up to the author opinion it doesn't express at all the physics behind the operation, for this reason we will try to avoid this expression in the document.}\\ 
We isolate the contribution of each degree of freedom $\uvec{x}_i$ in the cell $K$ to the integral balance that we have in that cell. The node residuals $\noderes$ are nothing more than the contributions of the nodes to the balance in the cell $K$ represented by $\elres$. This fact is expressed by the conservation relation \ref{conservation} that the node residuals must fulfil 
$$\sum_{\uvec{x}_i \in K} \noderes=\elres \quad \forall K \in \tess \quad \forall t  \in [0,T].$$
\item[iii)] \textbf{Imposition of a staggered balance at the nodes}\\
Once we isolated the node contributions to the element balance in each cell we impose a new "staggered equilibrium" at the nodes. The sum of the contributions that each node offers to the elements to which it belongs is $0$ i.e.
$$\sum_{K \in K_i} \noderes=\uvec{0} \quad t \in [0,T] \quad \forall \i=1,2,\dots,I$$
This is indeed a reasonable conservation requirement: nothing is created or destroyed at the nodes. The global contribution of each node to all the balances of all elements that share it is $0$.
\end{itemize}
The three steps are represented in figure \ref{RD}.

\begin{figure}[hp]
\begin{subfigure}{.9\textwidth}
  \centering
  % include first image
  \includegraphics[width=1\linewidth]{RD1.pdf}  
  \caption{Balance at the element $K$.}
  \label{RD1}
\end{subfigure}
\begin{subfigure}{.9\textwidth}
  \centering
  % include second image
  \includegraphics[width=1\linewidth]{RD2.pdf}  
  \caption{Isolation of the contributions of the nodes to the element balance.}
  \label{RD2}
\end{subfigure}
\begin{subfigure}{.8\textwidth}
  \centering
  % include second image
  \includegraphics[width=1\linewidth]{RD3tris.pdf}  
  \caption{Staggered balance at the node $i_7$.}
  \label{RD2}
\end{subfigure}
\caption{The three steps of the Residual Distribution method.}
\label{RD}
\end{figure}
Up to now the method is very general and also very abstract since we didn't specify how to define the node residuals $\noderes$. The features of the particular Residual Distribution method depend on the choice of the node residuals.
\begin{observation}
We have neglected the boundary conditions due to the huge variety of existing ones and the different ways to implement them. We just say that whenever they are not imposed strongly, i.e. fixing the coefficients $\uvec{c}_i(t)$ for the nodes belonging to the boundary, they are usually taken into account by defining some boundary residuals which are added in the balances of the nodes on the boundary. This is for example what is done for the inflow-outflow boundary conditions. Anyway the structure of the Deferred Correction applied to the Residual Distribution method as well as the proofs of the properties of the operators $\lopd^1$ and $\lopd^2$ that we will define doesn't change much when we include the boundary conditions.
\end{observation}

\section{Some examples of node residuals}

We will now present some examples of suitable node residuals and show that they fulfil the conservation property \ref{conservation}.


\begin{itemize}

\item[•] \textbf{Lax-Friedrichs}\\
$$\noderes=\int_K \varphi_i(\uvec{x})\frac{\partial}{\partial t}\uvec{u}_h(\uvec{x},t) d \uvec{x}+\int_{\partial{K}} \varphi_i(\uvec{x})\uvec{F}(\uvec{u}_h(\uvec{x},t))\cdot \uvec{\nu} d\sigma(\uvec{x})-\int_K \uvec{F}(\uvec{u}_h(\uvec{x},t))\cdot \nabla_{\uvec{x}} \varphi_i(\uvec{x}) d\uvec{x}+$$
\begin{equation}
\label{Lax-Friedrichs}
+\alpha_K(\uvec{u}_h)\left(\uvec{c}_i(t)-\overline{\uvec{c}}_K(t) \right)-\int_K \uvec{S}(\uvec{x},t,\uvec{u}_h(\uvec{x},t)) \varphi_i(\uvec{x}) d\uvec{x} \quad t\in [0,T]
\end{equation}
where $\overline{\uvec{c}}_K(t)$ is the average of the coefficients of the approximated solution associated to the nodes belonging to the cell $K$ i.e.
$$\overline{\uvec{c}}_K(t)=\frac{1}{\#\lbrace\uvec{x}_i \in K\rbrace}\sum_{\uvec{x}_i \in K}\uvec{c}_i(t)$$
and $\alpha_K(\uvec{u}_h)$ is defined through the Jacobian matrix of the flux in the following way
$$\alpha_K(\uvec{u}_h)=\sup\limits_{\substack{\uvec{x}\in f \\ f\subset \partial{K}}}\rho \left( \frac{\partial \uvec{F}}{\partial \uvec{u}}(\uvec{u}_h) \nu \right)$$ 
where $\rho\left( \frac{\partial \uvec{F}}{\partial \uvec{u}}(\uvec{u}_h) \nu \right)$ is the spectral radius of the matrix $ \frac{\partial \uvec{F}}{\partial \uvec{u}}(\uvec{u}_h) \nu $ i.e. the supremum of the absolute values of its eigenvalues and $f$ is a generic $D-1$-dimensional face of the generic element $K$ of the tessellation;



\item[•] \textbf{SUPG}\\
$$\noderes=\int_K \varphi_i(\uvec{x})\frac{\partial}{\partial t}\uvec{u}_h(\uvec{x},t) d \uvec{x}+\int_{\partial{K}} \varphi_i(\uvec{x})\uvec{F}(\uvec{u}_h(\uvec{x},t))\cdot \uvec{\nu} d\sigma(\uvec{x})-\int_K \uvec{F}(\uvec{u}_h(\uvec{x},t))\cdot \nabla_{\uvec{x}} \varphi_i(\uvec{x}) d\uvec{x}+$$
$$+h_K \int_K \left[\frac{\partial \uvec{F}}{\partial \uvec{u}}(\uvec{u}_h(\uvec{x},t))\cdot \nabla_{\uvec{x}}\varphi_i(\uvec{x})\right] \tau \left[\frac{\partial \uvec{F}}{\partial \uvec{u}}(\uvec{u}_h(\uvec{x},t))\cdot \nabla_{\uvec{x}}\uvec{u}_h(\uvec{x},t)\right]d\uvec{x}+$$
\begin{equation}
\label{SUPG}
-\int_K \uvec{S}(\uvec{x},t,\uvec{u}_h(\uvec{x},t)) \varphi_i(\uvec{x}) d\uvec{x} \quad t\in [0,T]
\end{equation}
where $h_K$ is a constant depending on the cell $K$ and $\tau\in \mathbb{R}^+$ is a positive parameter;


\item[•] \textbf{Burman}\\
$$\noderes=\int_K \varphi_i(\uvec{x})\frac{\partial}{\partial t}\uvec{u}_h(\uvec{x},t) d \uvec{x}+\int_{\partial{K}} \varphi_i(\uvec{x})\uvec{F}(\uvec{u}_h(\uvec{x},t))\cdot \uvec{\nu} d\sigma(\uvec{x})-\int_K \uvec{F}(\uvec{u}_h(\uvec{x},t))\cdot \nabla_{\uvec{x}} \varphi_i(\uvec{x}) d\uvec{x}+$$
$$+\sum_{f\subset\partial{K}}\theta h_f^2\int_f [\![ \nabla_{\uvec{x}} \uvec{u}_h(\uvec{x},t) ]\!] \cdot \nabla_{\uvec{x}} \varphi_i\vert_K(\uvec{x})  d\sigma(\uvec{x})+$$
\begin{equation}
\label{Burman}
-\int_K \uvec{S}(\uvec{x},t,\uvec{u}_h(\uvec{x},t)) \varphi_i(\uvec{x}) d\uvec{x} \quad t\in [0,T]
\end{equation}
where $f$ is a generic $D-1$-dimensional face of the generic element $K$, $\theta\in \mathbb{R^+}$ is a positive parameter, $h_f$ is a constant depending on the face $f$, $[\![ \cdot ]\!]$ represents the jump of the argument across $f$ i.e.
$$[\![ g(\uvec{x}) ]\!]=g\vert_K(\uvec{x})-g\vert_{K^+}(\uvec{x}) \quad \forall x \in f$$
with $K^+$ the element of the tessellation sharing the face $f$ with $K$ and $\cdot\vert_K$ and $\cdot\vert_{K^+}$ representing the restrictions to $K$ and $K^+$ thus
$$g\vert_K(\uvec{x})=\lim\limits_{\substack{\uvec{y}\rightarrow\uvec{x} \\ \uvec{y}\in K}}g(\uvec{y})$$ 
$$g\vert_{K^+}(\uvec{x})=\lim\limits_{\substack{\uvec{y}\rightarrow\uvec{x} \\ \uvec{y}\in K^{+}}}g(\uvec{y})$$
for any $x \in f$;
\item[•] \textbf{Non-variational node residuals}
\begin{equation}
\label{nonvar}
\noderes=\beta^K_i(\uvec{u}_h) \elres \quad t\in [0,T]
\end{equation}
where $\beta^K_i(\uvec{u}_h) \quad K\in \tess \quad \uvec{x}_i \in K$ are some coefficients depending on the approximated solution verifying 
\begin{equation}
\label{coefficients}
\sum_{\uvec{x}_i \in K}\beta^K_i(\uvec{u}_h)=1 \quad \forall K\in \tess.
\end{equation}
\end{itemize}
Let's verify that the conservation relation \ref{conservation} holds for all these types of node residuals.
For what concerns the last type of node residuals this follows trivially from the condition \ref{coefficients} imposed on the coefficients $\beta^K_i(\uvec{u}_h)$ in fact
$$\sum_{\uvec{x}_i \in K}\noderes=\sum_{\uvec{x}_i \in K}\beta^K_i(\uvec{u}_h)\elres=\left(\sum_{\uvec{x}_i \in K}\beta^K_i(\uvec{u}_h)\right)\elres=\elres.$$
For the other node residuals let's refer to the next property.



\begin{proposition}
The node residuals Lax-Friedrichs, SUPG and Burman given respectively in equations \ref{Lax-Friedrichs}, \ref{SUPG} and \ref{Burman} fulfil the conservation relation \ref{conservation}.
\end{proposition}
\begin{proof}
For all of them it follows from the normalization \ref{normalization} that we have made on the test functions which is recalled here
$$\sum_{i=1}^I\varphi_i(\uvec{x}) \equiv 1 \quad \forall \uvec{x} \in \overline{\Omega}.$$
In order to verify that the conservation relation holds let's observe that they all can be written in the form
$$\noderes=\int_K \varphi_i(\uvec{x})\frac{\partial}{\partial t}\uvec{u}_h(\uvec{x},t) d \uvec{x}+\int_{\partial{K}} \varphi_i(\uvec{x})\uvec{F}(\uvec{u}_h(\uvec{x},t))\cdot \uvec{\nu} d\sigma(\uvec{x})-\int_K \uvec{F}(\uvec{u}_h(\uvec{x},t))\cdot \nabla_{\uvec{x}} \varphi_i(\uvec{x}) d\uvec{x}+$$
\begin{equation}
\label{stabilizedgalerkin}
-\int_K \uvec{S}(\uvec{x},t,\uvec{u}_h(\uvec{x},t)) \varphi_i(\uvec{x}) d\uvec{x}+\ST(\uapp) \quad t\in [0,T]
\end{equation}
where $\ST(\uapp)$ is a stabilization term depending on the approximated solution $\uapp$.
We have 
\begin{itemize}
\item[•] \textbf{Lax-Friedrichs}
$$\ST(\uapp)=\alpha_K(\uvec{u}_h)\left(\uvec{c}_i(t)-\overline{\uvec{c}}_K(t) \right) \quad t\in [0,T];$$
\item[•] \textbf{SUPG}
$$\ST(\uapp)=h_K \int_K \left[\frac{\partial \uvec{F}}{\partial \uvec{u}}(\uvec{u}_h(\uvec{x},t))\cdot \nabla_{\uvec{x}}\varphi_i(\uvec{x})\right] \tau \left[\frac{\partial \uvec{F}}{\partial \uvec{u}}(\uvec{u}_h(\uvec{x},t))\cdot \nabla_{\uvec{x}}\uvec{u}_h(\uvec{x},t)\right]d\uvec{x} \quad t\in [0,T];$$
\item[•] \textbf{Burman}
$$\ST(\uapp)=\sum_{f\subset\partial{K}}\theta h_f^2\int_f [\![ \nabla_{\uvec{x}} \uvec{u}_h(\uvec{x},t) ]\!]  \cdot \nabla_{\uvec{x}} \varphi_i\vert_K(\uvec{x}) d\sigma(\uvec{x})\quad t\in [0,T].$$

\end{itemize}
If we make now the sum of these nodal residuals associated to the nodes of a cell $K$ we get
$$\sum_{\uvec{x}_i \in K} \noderes=\sum_{\uvec{x}_i \in K}\Biggl[ \int_K \varphi_i(\uvec{x})\frac{\partial}{\partial t}\uvec{u}_h(\uvec{x},t) d \uvec{x}+\int_{\partial{K}} \varphi_i(\uvec{x})\uvec{F}(\uvec{u}_h(\uvec{x},t))\cdot \uvec{\nu} d\sigma(\uvec{x})+$$
$$-\int_K \uvec{F}(\uvec{u}_h(\uvec{x},t))\cdot \nabla_{\uvec{x}} \varphi_i(\uvec{x}) d\uvec{x}-\int_K \uvec{S}(\uvec{x},t,\uvec{u}_h(\uvec{x},t)) \varphi_i(\uvec{x}) d\uvec{x}+\ST(\uapp)\Biggr].$$
Due to the fact that we are dealing with a finite sum we can enter the sum under the integrals and, thanks to basic analysis, get
$$\sum_{\uvec{x}_i \in K} \noderes= \int_K \left(\sum_{\uvec{x}_i \in K}\varphi_i(\uvec{x})\right)\frac{\partial}{\partial t}\uvec{u}_h(\uvec{x},t) d \uvec{x}+\int_{\partial{K}}\left(\sum_{\uvec{x}_i \in K}\varphi_i(\uvec{x})\right)\uvec{F}(\uvec{u}_h(\uvec{x},t))\cdot \uvec{\nu} d\sigma(\uvec{x})+$$ 

$$-\int_K  \uvec{F}(\uvec{u}_h(\uvec{x},t))\cdot \nabla_{\uvec{x}}\left(\sum_{\uvec{x}_i \in K}  \varphi_i(\uvec{x})\right) d\uvec{x}-\int_K \ \uvec{S}(\uvec{x},t,\uvec{u}_h(\uvec{x},t))\left(\sum_{\uvec{x}_i \in K}\varphi_i(\uvec{x})\right) d\uvec{x}+ \sum_{\uvec{x}_i \in K}\ST(\uapp).$$
Now we have the crucial point: since, as we already said, each basis function $\varphi_i$ has its support contained in the union of the elements containing the node to which it is associated then $\sum_{i=1}^I\varphi_i(\uvec{x}) \equiv 1 \quad \forall \uvec{x} \in \overline{\Omega}$ implies
$$\sum_{\uvec{x}_i \in K}  \varphi_i(\uvec{x}) \equiv 1 \quad \forall x \in K$$
because only the basis functions $\varphi_i$ s.t. the associated nodes $\uvec{x}_i \in K$ are not identically zero in $K$. And obviously since the gradient of a constant function is identically $0$ we can write

\begin{equation}
\label{gradient}
\nabla_{\uvec{x}}\left(\sum_{\uvec{x}_i \in K}  \varphi_i(\uvec{x})\right)\equiv 0 \quad \forall x \in K.
\end{equation}
Thus we get
$$\sum_{\uvec{x}_i \in K} \noderes= \int_K \frac{\partial}{\partial t}\uvec{u}_h(\uvec{x},t) d \uvec{x}+\int_{\partial{K}}\uvec{F}(\uvec{u}_h(\uvec{x},t))\cdot \uvec{\nu} d\sigma(\uvec{x})-\int_K \ \uvec{S}(\uvec{x},t,\uvec{u}_h(\uvec{x},t)) d\uvec{x}+ \sum_{\uvec{x}_i \in K}\ST(\uapp)$$
which, recalling the definition \ref{elementres} of the element residuals, reads
\begin{equation}
\label{intermediate}
\sum_{\uvec{x}_i \in K}\noderes=\elres+\sum_{\uvec{x}_i \in K}\ST(\uapp) \quad t\in [0,T].
\end{equation}
All we need to prove now is that
$$\sum_{\uvec{x}_i \in K}\ST(\uapp)\equiv\uvec{0}\quad \forall t\in [0,T].$$
Thanks to basic analytical passages based on linearity and on the fact that we are dealing with finite sums we get
\begin{itemize}

\item[•] \textbf{Lax-Friedrichs}
$$\sum_{\uvec{x}_i \in K}\ST(\uapp)=\sum_{\uvec{x}_i \in K}\alpha_K(\uvec{u}_h)\left(\uvec{c}_i(t)-\overline{\uvec{c}}_K(t) \right)=$$
$$=\alpha_K(\uvec{u}_h)\sum_{\uvec{x}_i \in K}\left( \uvec{c}_i(t)-\overline{\uvec{c}}_K(t) \right)=\alpha_K(\uvec{u}_h)\left(\sum_{\uvec{x}_i \in K} \uvec{c}_i(t)-\sum_{\uvec{x}_i \in K}\overline{\uvec{c}}_K(t) \right)=$$
$$=\alpha_K(\uvec{u}_h)\bigl[   \#\lbrace\uvec{x}_i \in K\rbrace \overline{\uvec{c}}_K(t)-\#\lbrace\uvec{x}_i \in K\rbrace \overline{\uvec{c}}_K(t)  \bigr] \equiv 0  \quad \forall t\in [0,T]$$
because of the definition of the average $\overline{\uvec{c}}_K(t)=\frac{1}{\#\lbrace\uvec{x}_i \in K\rbrace}\sum_{\uvec{x}_i \in K}\uvec{c}_i(t);$
\item[•] \textbf{SUPG}
$$\sum_{\uvec{x}_i \in K}\ST(\uapp)=\sum_{\uvec{x}_i \in K}h_K \int_K \left[\frac{\partial \uvec{F}}{\partial \uvec{u}}(\uvec{u}_h(\uvec{x},t))\cdot \nabla_{\uvec{x}}\varphi_i(\uvec{x})\right] \tau \left[\frac{\partial \uvec{F}}{\partial \uvec{u}}(\uvec{u}_h(\uvec{x},t))\cdot \nabla_{\uvec{x}}\uvec{u}_h(\uvec{x},t)\right]d\uvec{x}=$$
$$h_K \int_K \left[\frac{\partial \uvec{F}}{\partial \uvec{u}}(\uvec{u}_h(\uvec{x},t))\cdot \nabla_{\uvec{x}}\left(\sum_{\uvec{x}_i \in K}\varphi_i(\uvec{x})\right)\right] \tau \left[\frac{\partial \uvec{F}}{\partial \uvec{u}}(\uvec{u}_h(\uvec{x},t))\cdot \nabla_{\uvec{x}}\uvec{u}_h(\uvec{x},t)\right]d\uvec{x}\equiv\uvec{0}\quad \forall t\in [0,T]$$
because $\sum_{\uvec{x}_i \in K}  \varphi_i(\uvec{x}) \equiv 1\quad \forall x \in K$ and so its gradient is identically $0$ in $K$ as we observed in \ref{gradient};
\item[•] \textbf{Burman}
$$\sum_{\uvec{x}_i \in K}\ST(\uapp)=\sum_{\uvec{x}_i \in K}\sum_{f\subset\partial{K}}\theta h_f^2\int_f [\![ \nabla_{\uvec{x}} \uvec{u}_h(\uvec{x},t) ]\!] \cdot \nabla_{\uvec{x}} \varphi_i\vert_K(\uvec{x}) d\sigma(\uvec{x})=$$
$$=\sum_{f\subset\partial{K}}\theta h_f^2\int_f [\![ \nabla_{\uvec{x}} \uvec{u}_h(\uvec{x},t) ]\!] \cdot \nabla_{\uvec{x}} \left( \sum_{\uvec{x}_i \in K}\varphi_i\vert_K(\uvec{x})\right) d\sigma(\uvec{x})\equiv\uvec{0}\quad \forall t\in [0,T]$$
again because of \ref{gradient} since we are considering the limit at the boundary of the restriction to $K$ of the gradient of the function $\sum_{\uvec{x}_i \in K}  \varphi_i(\uvec{x})$ which is constant in $K$ and so
$$\nabla_{\uvec{x}} \left( \sum_{\uvec{x}_i \in K}\varphi_i\vert_K(\uvec{x})\right)=\nabla_{\uvec{x}} \left( \sum_{\uvec{x}_i \in K} \lim\limits_{\substack{\uvec{y}\rightarrow\uvec{x} \\ \uvec{y}\in K}}\varphi_i(\uvec{y})\right)=$$
$$=\nabla_{\uvec{x}} \left(  \lim\limits_{\substack{\uvec{y}\rightarrow\uvec{x} \\ \uvec{y}\in K}}\sum_{\uvec{x}_i \in K} \varphi_i(\uvec{y})\right)=\nabla_{\uvec{x}}1\equiv 0 \quad \forall x \in f \quad \forall f\subset \partial{K}.$$  
\end{itemize}
\end{proof}

\section{A focus on the balance equations and on the mass matrix}
We will now go deeper into the details of the method. Let's focus on the balance \ref{nodebalance} that we recall here
$$\sum_{K \in K_i} \noderes=\uvec{0} \quad t \in [0,T] \quad \forall \i=1,2,\dots,I.$$
We will write explicitely the equations of this system of ODEs for the mentioned node residuals.
If we use Lax-Friedrichs, SUPG and Burman node residuals i.e. \ref{Lax-Friedrichs}, \ref{SUPG} and \ref{Burman}, which as already shown can all be written in the form \ref{stabilizedgalerkin}, the balance \ref{nodebalance} at the nodes reads
$$\uvec{0}=\sum_{K\in K_i} \noderes=\sum_{K\in K_i}\Biggl[ \int_K \varphi_i(\uvec{x})\frac{\partial}{\partial t}\uvec{u}_h(\uvec{x},t) d \uvec{x}+\int_{\partial{K}} \varphi_i(\uvec{x})\uvec{F}(\uvec{u}_h(\uvec{x},t))\cdot \uvec{\nu} d\sigma(\uvec{x})+$$
$$-\int_K \uvec{F}(\uvec{u}_h(\uvec{x},t))\cdot \nabla_{\uvec{x}} \varphi_i(\uvec{x}) d\uvec{x}-\int_K \uvec{S}(\uvec{x},t,\uvec{u}_h(\uvec{x},t)) \varphi_i(\uvec{x}) d\uvec{x}+\ST(\uapp)\Biggr] \quad t \in [0,T] \quad \forall \i=1,2,\dots,I.$$
By substituiting $\uapp(\uvec{x},t)=\sum_{j=1}^I\uvec{c}_j(t)\varphi_j(\uvec{x})$ in the previous equation we get
$$\uvec{0}=\sum_{K\in K_i}\Biggl[ \int_K \varphi_i(\uvec{x})\frac{\partial}{\partial t} \left( \sum_{j=1}^I\uvec{c}_j(t) \varphi_j(\uvec{x}) \right) d \uvec{x}+\int_{\partial{K}} \varphi_i(\uvec{x})\uvec{F}(\sum_{j=1}^I\uvec{c}_j(t) \varphi_j(\uvec{x}))\cdot \uvec{\nu} d\sigma(\uvec{x})+$$
$$-\int_K \uvec{F}(\sum_{j=1}^I\uvec{c}_j(t) \varphi_j(\uvec{x}))\cdot \nabla_{\uvec{x}} \varphi_i(\uvec{x}) d\uvec{x}-\int_K \uvec{S}(\uvec{x},t,\sum_{j=1}^I\uvec{c}_j(t) \varphi_j(\uvec{x})) \varphi_i(\uvec{x}) d\uvec{x}+$$
$$+\ST(\sum_{j=1}^I\uvec{c}_j(t) \varphi_j(\uvec{x}))\Biggr]\quad t \in [0,T] \quad \forall \i=1,2,\dots,I.$$
We remark that the basis function $\varphi_j$ has support in the union of the elements containing the node $\uvec{x}_j$ to which it is associated so the only basis functions having support in $K$ are the ones corresponding to the nodes belonging to $K$ thus we can write 
$$\uvec{0}=\sum_{K\in K_i}\Biggl[ \int_K \varphi_i(\uvec{x})\frac{\partial}{\partial t} \left( \sum_{\uvec {x}_j \in K}\uvec{c}_j(t) \varphi_j(\uvec{x}) \right) d \uvec{x}+\int_{\partial{K}} \varphi_i(\uvec{x})\uvec{F}(\sum_{j=1}^I\uvec{c}_j(t) \varphi_j(\uvec{x}))\cdot \uvec{\nu} d\sigma(\uvec{x})+$$
$$-\int_K \uvec{F}(\sum_{j=1}^I\uvec{c}_j(t) \varphi_j(\uvec{x}))\cdot \nabla_{\uvec{x}} \varphi_i(\uvec{x}) d\uvec{x}-\int_K \uvec{S}(\uvec{x},t,\sum_{j=1}^I\uvec{c}_j(t) \varphi_j(\uvec{x})) \varphi_i(\uvec{x}) d\uvec{x}+$$
$$+\ST(\sum_{j=1}^I\uvec{c}_j(t) \varphi_j(\uvec{x}))\Biggr]\quad t \in [0,T] \quad \forall \i=1,2,\dots,I.$$
Thanks to basic analytic passages the previous nodal balances become
$$\uvec{0}=\sum_{K\in K_i}\sum_{\uvec {x}_j \in K} \left(\int_K \varphi_i(\uvec{x}) \varphi_j(\uvec{x}) d \uvec{x}\right)\frac{d}{d t} \uvec{c}_j(t)+\sum_{K\in K_i}\Biggl[\int_{\partial{K}} \varphi_i(\uvec{x})\uvec{F}(\sum_{j=1}^I\uvec{c}_j(t) \varphi_j(\uvec{x}))\cdot \uvec{\nu} d\sigma(\uvec{x})+$$
$$-\int_K \uvec{F}(\sum_{j=1}^I\uvec{c}_j(t) \varphi_j(\uvec{x}))\cdot \nabla_{\uvec{x}} \varphi_i(\uvec{x}) d\uvec{x}-\int_K \uvec{S}(\uvec{x},t,\sum_{j=1}^I\uvec{c}_j(t) \varphi_j(\uvec{x})) \varphi_i(\uvec{x}) d\uvec{x}+$$
\begin{equation}
\label{RDGaldiss}
+\ST(\sum_{j=1}^I\uvec{c}_j(t) \varphi_j(\uvec{x}))\Biggr]\quad t \in [0,T] \quad \forall \i=1,2,\dots,I.
\end{equation}




This is the system of ODEs that we get with the Residual Distribution method and Lax-Friedrichs, SUPG and Burman node residuals. We remind that the stabilization terms $\ST$ depend on the particular node residuals chosen. The first term in \ref{RDGaldiss} "generates" the mass matrix $\massmatrix \in \mathbb{R}^{(I\times N)\times(I\times N)}$ which, in this case, is positive defined and thus nonsingular but unfortunately sparse\footnote{This means that we cannot rely on the efficient algorithm designed for band matrices.}.
If we use insted the non-variational node residuals given by \ref{nonvar} we get
$$\uvec{0}=\sum_{K\in K_i} \noderes=\sum_{K\in K_i}\beta^K_i(\uvec{u}_h) \elres=$$
$$=\sum_{K\in K_i}\beta^K_i(\uvec{u}_h)\Biggl[ \int_K   \frac{\partial}{\partial t}\uapp(\uvec{x},t)d\uvec{x}+\int_{\partial{K}} \mathbf{F}(\uapp(\uvec{x},t))\cdot\nu d\sigma(\uvec{x})+$$
$$-\int_K\mathbf{S}(\uvec{x},t,\uapp(\uvec{x},t)) d\uvec{x}\Biggr] \quad t \in [0,T] \quad \forall \i=1,2,\dots,I.$$
Substituiting again $\uapp(\uvec{x},t)=\sum_{j=1}^I\uvec{c}_j(t)\varphi_j(\uvec{x})$ we have
$$\uvec{0}=\sum_{K\in K_i}\beta^K_i(\sum_{j=1}^I\uvec{c}_j(t)\varphi_j(\uvec{x}))\Biggl[\int_K     \frac{\partial}{\partial t} \left(\sum_{j=1}^I\uvec{c}_j(t)\varphi_j(\uvec{x})\right)d \uvec{x}+$$
$$+   \int_{\partial{K}} \mathbf{F}(\sum_{j=1}^I\uvec{c}_j(t)\varphi_j(\uvec{x}))\cdot\nu d\sigma(\uvec{x})-\int_K\mathbf{S}(\uvec{x},t,\sum_{j=1}^I\uvec{c}_j(t)\varphi_j(\uvec{x})) d\uvec{x}\Biggr] \quad t \in [0,T] \quad \forall \i=1,2,\dots,I$$
and relying again on the fact that the only basis functions having support in $K$ are the ones corresponding to the nodes belonging to $K$ we get
$$\uvec{0}=\sum_{K\in K_i}\beta^K_i(\sum_{j=1}^I\uvec{c}_j(t)\varphi_j(\uvec{x}))\Biggl[\int_K     \frac{\partial}{\partial t} \left(\sum_{\uvec{x}_j\in K}\uvec{c}_j(t)\varphi_j(\uvec{x})\right)d \uvec{x}+$$
$$+   \int_{\partial{K}} \mathbf{F}(\sum_{j=1}^I\uvec{c}_j(t)\varphi_j(\uvec{x}))\cdot\nu d\sigma(\uvec{x})-\int_K\mathbf{S}(\uvec{x},t,\sum_{j=1}^I\uvec{c}_j(t)\varphi_j(\uvec{x})) d\uvec{x}\Biggr] \quad t \in [0,T] \quad \forall \i=1,2,\dots,I$$
Thanks to basic manipulations the previous node balances become
$$\uvec{0}=\sum_{K\in K_i}\sum_{\uvec{x}_j \in K}\Biggl[\beta^K_i(\sum_{j=1}^I\uvec{c}_j(t)\varphi_j(\uvec{x}))\int_K \varphi_j(\uvec{x})d\uvec{x}\Biggr]\frac{d}{d t}\uvec{c}_j(t)+$$
$$+\sum_{K\in K_i}\beta^K_i(\sum_{j=1}^I\uvec{c}_j(t)\varphi_j(\uvec{x}))\Biggl[\int_{\partial{K}} \mathbf{F}(\sum_{j=1}^I\uvec{c}_j(t)\varphi_j(\uvec{x}))\cdot\nu d\sigma(\uvec{x})+$$
\begin{equation}
\label{RDnonvar}
-\int_K \mathbf{S}(\uvec{x},t,\sum_{j=1}^I\uvec{c}_j(t)\varphi_j(\uvec{x}))    d\uvec{x}\Biggr] \quad t \in [0,T] \quad \forall \i=1,2,\dots,I.
\end{equation}
This is the system of ODEs that we get with the Residual Distribution method and non-variational node residuals.
Also in this case the mass matrix is given by the first term of the balance equation. In this case we do not even know if it is invertible or not and as, the reader may immediately notice, it depends on the coefficients $\uvec{c}_i(t)$ through the coefficients $\beta^K_i$ which take as argument the approximated solution $\uapp=\sum_{j=1}^I\uvec{c}_j(t)\varphi_j(\uvec{x})$.
If in the first case avoiding to invert the mass matrix is just a matter of computational efficiency, now it is a must: even if we assume that it is nonsingular (which is not guaranteed), a traditional approach involving the inversion would imply at each time step to recompute (and invert) the mass matrix resulting in prohibitive computational times.



%Example of the system of equations DONE

%BCs -> inflow BCs NO

%div therorem DONE



\section{On the derivation of Lax-Friedrichs, SUPG and Burman node residuals}


Let's make some observations on the derivations of the first three node residuals: Lax-Friedrichs, SUPG and Burman.
The Residual Distribution schemes that we get by using these residuals are equivalent to a continuous Galerkin FEM scheme with some diffusion to avoid the pathologies of a central scheme.
We will talk more in detail about the Galerkin finite elements method, or Galerkin FEM, later in the context of the Dicontinuous Galerkin schemes, but it's better to introduce some concepts now in order to have a clearer understanding of the meaning of the three mentioned node residuals.
Basically the Galerkin FEM for hyperbolic systems of balance laws consists in a projection of the analytical weak formulation in space over a finite dimensional space. 
Let's thus derive the weak formulation in space by multiplying our initial equation \ref{sys} by a spatial test function with compact support and integrating in space  
$$\int_{\Omega} \left(\frac{\partial}{\partial t}\uvec{u}(\uvec{x},t)+div_{\uvec{x}} \mathbf{F}(\uvec{u}(\uvec{x},t))\right)\varphi(\uvec{x}) d\uvec{x} =\int_{\Omega} \mathbf{S}(\uvec{x},t,\uvec{u}(\uvec{x},t))\varphi(\uvec{x}) d\uvec{x}  \quad \forall \varphi \in C^1_o(\overline{\Omega}).$$
If we apply the divergence theorem we get 
$$\int_{\Omega} \varphi(\uvec{x}) \frac{\partial}{\partial t}\uvec{u}(\uvec{x},t)  d\uvec{x}+ \int_{\partial \Omega} \varphi(\uvec{x})\mathbf{F}(\uvec{u}(\uvec{x},t))\cdot\uvec{\nu}d\sigma(\uvec{x})+$$
\begin{equation}
\label{weakspace}
- \int_{\Omega} \mathbf{F}(\uvec{u}(\uvec{x},t)) \cdot \nabla_{\uvec{x}} \varphi(\uvec{x}) d\uvec{x}=\int_{\Omega} \mathbf{S}(\uvec{x},t,\uvec{u}(\uvec{x},t))\varphi(\uvec{x},t) d\uvec{x}  \quad \forall \varphi \in C^1_o(\overline{\Omega})
\end{equation}
which is the weak formulation only in space. Now, according to the Galerkin FEM phylosophy, we project \ref{weakspace} over the finite dimensional subspace $(V_h)^N$.
In particular, given a conformal tessellation $\tess$ made of simplices\footnote{Again we remark that this assumption is just meant to make things easier but it is not mandatory.}, we adopt the usual discretization for the approximated solution
$$\uapp(\uvec{x},t)=\sum_{i=1}^I\uvec{c}_i(t)\varphi_i(\uvec{x}) \quad \forall(\uvec{x},t)\in \overline{\Omega}\times [0,T]$$
i.e. we assume at any time $t \in [0,T]$ the approximated solution $\uapp$ to belong to the finite dimensional space $(V_h)^N$ with $V_h=span\lbrace \varphi_i\rbrace_{i=1,2,\dots,I}$. The spatial basis functions $\varphi_i$ are very general: they do not have to be mandatorily polynomials or continuous\footnote{If the basis functions are continuous we get a continuous Galerkin FEM, otherwise we get a discontinuous Galerkin FEM.} but since we are trying to show the link between this approach and the Residual Distribution schemes with Lax-Friedrichs, SUPG and Burman node residuals we will consider the same space $V_h$ as the one considered in the Residual Distribution method i.e. the space of continuous, piecewise polynomial functions
$$V_h=\lbrace g \in C^0(\overline{\Omega}) \quad s.t. \quad g\vert_K \in \mathbb{P}_M \quad \forall K \in \tess\rbrace.$$
Let's remind that each function $\varphi_i$ is associated to a node $\uvec{x}_i \in \overline{\Omega}$ where its value is not $0$ and has support in the union of the elements of the tessellation containing that node.
Therefore as we anticipated we project \ref{weakspace} over the finite dimensional space $(V_h)^N$ i.e. we look for $\uapp(\uvec{x},t)=\sum_{j=1}^I\uvec{c}_j(t)\varphi_j(\uvec{x}) \quad t\in [0,T]$ s.t.

$$\int_{\Omega} \varphi(\uvec{x}) \frac{\partial}{\partial t}\uapp(\uvec{x},t)  d\uvec{x}+ \int_{\partial \Omega} \varphi(\uvec{x})\mathbf{F}(\uapp(\uvec{x},t))\cdot\uvec{\nu}d\sigma(\uvec{x})+$$
$$
- \int_{\Omega} \mathbf{F}(\uapp(\uvec{x},t)) \cdot \nabla_{\uvec{x}} \varphi(\uvec{x}) d\uvec{x}=\int_{\Omega} \mathbf{S}(\uvec{x},t,\uapp(\uvec{x},t))\varphi(\uvec{x},t) d\uvec{x}  \quad \forall \varphi \in V_h.
$$
Due to linearity this is equivalent to require
$$\int_{\Omega} \varphi_i(\uvec{x}) \frac{\partial}{\partial t}\uapp(\uvec{x},t)  d\uvec{x}+ \int_{\partial \Omega} \varphi_i(\uvec{x})\mathbf{F}(\uapp(\uvec{x},t))\cdot\uvec{\nu}d\sigma(\uvec{x})+$$
$$
- \int_{\Omega} \mathbf{F}(\uapp(\uvec{x},t)) \cdot \nabla_{\uvec{x}} \varphi_i(\uvec{x}) d\uvec{x}=\int_{\Omega} \mathbf{S}(\uvec{x},t,\uapp(\uvec{x},t))\varphi_i(\uvec{x},t) d\uvec{x}  \quad \forall i=1,2,\dots,I.
$$
If we now substitute $\uapp(\uvec{x},t)=\sum_{j=1}^I\uvec{c}_j(t)\varphi_j(\uvec{x})$ in the previous equation we get
$$\int_{\Omega} \varphi_i(\uvec{x}) \frac{\partial}{\partial t}\left( \sum_{j=1}^I\uvec{c}_j(t)\varphi_j(\uvec{x})\right)  d\uvec{x}+ \int_{\partial \Omega} \varphi_i(\uvec{x})\mathbf{F}(\sum_{j=1}^I\uvec{c}_j(t)\varphi_j(\uvec{x}))\cdot\uvec{\nu}d\sigma(\uvec{x})+$$
$$
- \int_{\Omega} \mathbf{F}(\sum_{j=1}^I\uvec{c}_j(t)\varphi_j(\uvec{x})) \cdot \nabla_{\uvec{x}} \varphi_i(\uvec{x}) d\uvec{x}=\int_{\Omega} \mathbf{S}(\uvec{x},t,\sum_{j=1}^I\uvec{c}_j(t)\varphi_j(\uvec{x}))\varphi_i(\uvec{x}) d\uvec{x}  \quad \forall j=1,2,\dots,I.
$$
and thus, thanks to basic analytic passages
$$\sum_{j=1}^I \left(\int_{\Omega} \varphi_i(\uvec{x}) \varphi_j(\uvec{x})  d\uvec{x}\right)\frac{d}{d t} \uvec{c}_j(t)=- \int_{\partial \Omega} \varphi_i(\uvec{x})\mathbf{F}(\sum_{j=1}^I\uvec{c}_j(t)\varphi_j(\uvec{x}))\cdot\uvec{\nu}d\sigma(\uvec{x})+$$
$$
+ \int_{\Omega} \mathbf{F}(\sum_{j=1}^I\uvec{c}_j(t)\varphi_j(\uvec{x})) \cdot \nabla_{\uvec{x}} \varphi_i(\uvec{x}) d\uvec{x}+\int_{\Omega} \mathbf{S}(\uvec{x},t,\sum_{j=1}^I\uvec{c}_j(t)\varphi_j(\uvec{x}))\varphi_i(\uvec{x}) d\uvec{x}  \quad \forall i=1,2,\dots,I.
$$
This is the semidiscrete formulation of the continuous Galerkin FEM method: a system of ODEs like \ref{semidiscr} characterized by a mass matrix $\massmatrix \in \mathbb{R}^{(I \times N) \times (I \times N)}$ positive defined but sparse. 
We will now show that this system of ODEs is equivalent to the one in \ref{RDGaldiss} i.e. the system got with the Residual Distribution method and Lax-Friedrichs, SUPG and Burman node residuals up to the presence of the stabilization terms.





  
%Derivation of the first three 
%Galerkin with stabilization term STILL

%Lax Friedrichs SUPG only first order accurate STILL

%Construction of the mass matrix STILL

%Remark ODE STILL


%No need for numerical flux STILL

%Examples STILL

%Boundary conditions: inflow - outflow NOOOOOOOOOOOOOOOO, DONE

%Associated to some node in which the value is not 0 DONE

%Accuracy of polynomials DONE

%u_h,t DONE




\chapter[The Deferred Correction in balance laws]{The Deferred Correction in balance laws as an explicit time integrator}











%For simplicity we will neglect the nodes on the boundary i.e. we will not consider the boundary residual. It would not be so difficult to include them in the discussion but it would be just a boring complication. The basic strategy and structure of the proof would not change.


%obs 

%This work is an attempt to give a systematic introduction to the Residual Distribution (RD in the following) technique. Despite its generality and flexibility it is probably the less used and known approach in the CFD community. The main reason for it is probably the fact that it is a very general approach multidimensional-oriented and it's very natural to present it in the general context of a multidimensional hyperbolic system on an unstructured grid rather than in straightforward cases. Paradoxically it lacks the possibility to be introduced in a "friendly" evnvironment.
%Another problem is the fact that it is a relatively new approach if compared with FV and FEM approaches, for this reason the literature is obviously less rich and many important results are missing expecially for what concerns stability. Despite this there are many pros: for example the possibility on an arbitrary high-order explicit formulation (see Davide, Remi, DeC) or the fact that it can be shown that continuous-FEM as well as Discontinuous Galerkin and Finite Volumes schemes can be seen as particoular cases of Residual Distribution schemes. 
%This is a personal elaboration and deliberately the notation differ from the existing literature.
%\chapter{Directly to the point}
%We have to deal with a general hyperbolic system
%$$\frac{\partial}{\partial t}\uvec{u}+div F = \uvec{S}$$
%on a domain $\Omega \times [0,T]$.
%
%
%
%We consider a classical continuous-FEM discretization of the domain and of the solution. So we consider a triangulation $\Omega_h$ of the spatial domain made by non-overlapping polyhedric subsets of $\Omega$ which completely cover $\Omega$ and a time discretization $0=t_0<t_1<\dots<t_f=T$. For simplicity we will consider a uniform time-dicretization i.e. $t_{j+1}-t_j=const$ but we can easily consider a more general discretization with non-equidistant nodes.
%As usual $h$ is the characteristic length of the mesh
%$$h=Sup_{K\in \Omega_h} diam(K).$$
%Morover, in the context of a semidiscrete formulation, we consider a general continuous approximation of the solution 
%$\uvec{u}_h=\sum_{j=1}^N \uvec{c_j}(t) \phi_j(x).$
%$N$ is the dimension of the space $V_h$ in which we approximate the solution at each time-step and it must be equal to the number of nodes of the spatial triangulation; ${\phi_j}_{j=1,2,\dots,N}$ is a set of continuous functions which constitutes a basis of this space, the coefficients $\uvec{c_j}$ just depend on time.
%We'll refer with the lecter $i$ to the nodes of the element $K$ of the triangulation. 
%The main ingredients of every RD schemes are 3
%\begin{itemize}
%\item[1)] \textbf{Cell residuals}\\
%For each cell $K$ of the triangulation we define the residual of the cell $\res^K$ as the integral of the equation over the cell
%$$\res^K(\uvec{u}_h)=\int_K \left[\frac{\partial}{\partial t}\uvec{u_h}+div F - \uvec{S}\right]$$ 
%\item[2)] \textbf{Node residuals}\\
%For each node $i$ of the cell $K$ we define the residual of the node
%$\res^K_i(\uvec{u}_h)$ in such a way that a conservation property is guaranteed
%$$\sum_{i\in K}\res^K_i(\uvec{u}_h)=\res^K(\uvec{u}_h)$$ 
%\item[3)] \textbf{Balance at the nodes}\\
%We impose
%$$\sum_{K\in K_i}\res^K_i(\uvec{u}_h)=0 \quad \forall i$$
%where $K_i$ is the set of the elements which contain the node $i$. This is in substance a nonlinear system of equations which allows us to advance in time.
%\end{itemize}
%\section{Physical sense}
%The \textbf{physical sense} is the following. If we consider a smooth solution $u$ of the equation we'd have the cell residuals equal to 0 because we'd have
%$$\frac{\partial}{\partial t}\uvec{u}+div F = \uvec{S} \quad \forall K \in \Omega_h$$  
%we are nothing more than \textbf{isolating the contribute} of each node to the cell residual and imposing the balance at each node.
%We are starting from an equilibrium condition  at the cells and imposing another "staggered" equilibrium.
%
%
%
%Up to this point we have defined a very general framework. As the reader may have noticed we haven't defined the nodes residuals, this is due to the fact that this definition is not "unique". The properties of the scheme depend on the way we define the residuals. If we choose properly the residuals we can obtain all the different schemes that we have mentioned (continuous FEM, Discontinuous Galerkin, FV) we we'll talk about it more in detail later.
%Let's now try to be more specific.
%In general we approximate (each component of) the solution in the space of the continuous functions with compact support in the closure of $\Omega$ for which the restriction to the generic element $K$ of the triangulation is a polynomial with a maximum degree equal to $r \in \mathbb{N}$, i.e.
%$$X_r^h=\lbrace v \in C^0(\bar{\Omega}) \quad v\vert_K \in \mathbb{P}_r \quad \forall K \in \Omega_h \rbrace$$
%\section{Some typical examples}
%For example two general ways to define the nodes residuals are
%\begin{itemize}
%\item[•] \textbf{Splitting with coefficients}\\
%We define for each cell $K$ and node $i$ some (uniformly) bounded coefficients $\beta^K_i$ s.t. $\sum_{i \in K}\beta^K_i=1$ and we set
%$$\res^K_i(\uvec{u}_h)=\beta_i^K \res^K(\uvec{u}_h) $$
%\item[•] \textbf{Splitting with test functions}\\
%We define for each cell $K$ and node $i$ some bounded test functions $\omega^K_i$ s.t. $\sum_{i \in K}\omega^K_i\equiv1$ 
%$$\res^K_i(\uvec{u}_h)=\int_K \omega^K_i \left[\frac{\partial}{\partial t}\uvec{u_h}+div F - \uvec{S}\right]  $$
%\end{itemize}
%In both cases it is very easy to verify that the conservation property $\sum_{i\in K}\res^K_i(\uvec{u}_h)=\res^K(\uvec{u}_h)$ holds. It directly follows from the properties that we have imposed on the coefficients and test functions $\sum_{i \in K}\beta^K_i=1$ and $\sum_{i \in K}\omega^K_i\equiv1$.
%$$\res^K_i(\uvec{u}_h)=\beta_i$$
%
%
%
%
%
%
%
%
%
%In general the coefficients $\beta^K_i$ depend on the approximated solution itself
%
%
%But we can also split 
%The coefficients lead to misu
%
%NB space residuals
%
%
%
%
%
%
%\chapter{Explicit high-order}
%
%\chapter{Other schemes}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%Indeed if we consider a smooth solution $u$ we have that it satisfies the equation in the whole domain so the quantity






\end{document}
